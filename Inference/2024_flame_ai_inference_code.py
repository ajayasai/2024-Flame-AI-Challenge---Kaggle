# -*- coding: utf-8 -*-
"""2024 FLAME AI Inference Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J-galapy1d7kelMZHku1iS14FSh91qUP
"""

import os
import subprocess
import importlib

required_packages = ['torch', 'torchvision', 'segmentation-models-pytorch', 'efficientnet_pytorch']

def install_packages(packages):
    for package in packages:
        try:
            # Check if the package is already installed
            importlib.import_module(package)
            print(f"{package} already installed.")
        except ImportError:
            print(f"Installing {package}...")
            subprocess.check_call(['pip', 'install', package])
            print(f"{package} installed successfully.")

install_packages(required_packages)

train_csv = '/kaggle/input/2024-flame-ai-challenge/dataset/train.csv'
train_data_dir = '/kaggle/input/2024-flame-ai-challenge/dataset/train'
test_csv = '/kaggle/input/2024-flame-ai-challenge/dataset/test.csv'
test_data_dir = '/kaggle/input/2024-flame-ai-challenge/dataset/test'

SEQ_LEN = 5
seq_len = SEQ_LEN
batch_size = 4
NUM_TSTEPS = 20   #Timesteps required to predict

# 1. Import Necessary Libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import (
    resnet18, resnet34, resnet50, resnet101, resnet152,
    ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights,
    densenet121, DenseNet121_Weights,
    mobilenet_v2, MobileNet_V2_Weights
)
from efficientnet_pytorch import EfficientNet  # Ensure this is installed
from torch.utils.data import DataLoader, Dataset, Subset
import pandas as pd
import numpy as np
import os
import torchvision.ops as ops  # For DeformConv2d

# Try importing MobileNetV3 from timm
try:
    import timm
    has_timm = True
except ImportError:
    has_timm = False
    print("timm library not found. MobileNetV3 will not be available. Please install it using `pip install timm`.")

class FireDataset_20(Dataset):
    def __init__(self, csv_data, data_dir, seq_len=5, multi_step=20, is_train=True):
        """
        Args:
            csv_data (str or DataFrame): Path to the CSV file or a pandas DataFrame.
            data_dir (str): Directory containing the .dat files.
            seq_len (int): Number of past timesteps to use as input.
            multi_step (int): Number of future timesteps to predict.
            is_train (bool): Flag indicating training or testing mode.
        """
        if isinstance(csv_data, str):
            self.data_info = pd.read_csv(csv_data)
        else:
            self.data_info = csv_data  # Accept DataFrame directly

        self.data_dir = data_dir
        self.seq_len = seq_len
        self.multi_step = multi_step
        self.is_train = is_train
        self.samples = self._create_samples()

    def _create_samples(self):
        samples = []
        for _, row in self.data_info.iterrows():
            id = row['id']
            u = row['u']
            alpha = row['alpha']
            Nt = row['Nt']
            # Load filenames
            theta_path = os.path.join(self.data_dir, row['theta_filename'])
            ustar_path = os.path.join(self.data_dir, row['ustar_filename'])
            xi_path = os.path.join(self.data_dir, row['xi_filename'])

            # Check if files exist
            if not os.path.exists(theta_path) or not os.path.exists(ustar_path) or not os.path.exists(xi_path):
                print(f"Missing files for ID {id}. Skipping.")
                continue

            # Load .dat files (assuming binary format)
            try:
                theta = np.fromfile(theta_path, dtype=np.float32).reshape(Nt, 113, 32)
                ustar = np.fromfile(ustar_path, dtype=np.float32).reshape(Nt, 113, 32)
                xi = np.fromfile(xi_path, dtype=np.float32).reshape(Nt, 113, 32)
            except ValueError as e:
                print(f"Error reshaping files for ID {id}: {e}")
                continue

            if self.is_train:
                # Ensure there are enough timesteps to create at least one sample
                required_timesteps = self.seq_len + self.multi_step
                if Nt < required_timesteps:
                    print(f"Not enough timesteps for ID {id}. Required: {required_timesteps}, Available: {Nt}")
                    continue

                # Create multiple samples using rolling window with step size NUM_STEPS - SEQ_LEN (15)
                #step_size = NUM_STEPS - self.seq_len  # 15
                step_size = 1

                for t in range(0, Nt - self.seq_len - self.multi_step + 1, step_size):
                    # Input sequences: theta, ustar, xi for seq_len timesteps
                    theta_seq = theta[t:t+self.seq_len]  # Shape: [seq_len, 113,32]
                    ustar_seq = ustar[t:t+self.seq_len]
                    xi_seq = xi[t:t+self.seq_len]

                    # Target sequences: xi for multi_step timesteps
                    target_seq = xi[t+self.seq_len:t+self.seq_len + self.multi_step]  # Shape: [multi_step, 113,32]

                    # Stack features per time step: [seq_len, 5, 113,32]
                    features = []
                    for i in range(self.seq_len):
                        theta_i = theta_seq[i]  # [113,32]
                        ustar_i = ustar_seq[i]
                        xi_i = xi_seq[i]
                        u_i = u
                        alpha_i = alpha

                        # Convert scalar features to tensors and tile
                        u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]
                        alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]

                        # Stack all features
                        feature = torch.stack([
                            torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]
                            torch.tensor(theta_i, dtype=torch.float32),  # [113,32]
                            torch.tensor(xi_i, dtype=torch.float32),     # [113,32]
                            u_tensor.squeeze(0),                        # [113,32]
                            alpha_tensor.squeeze(0)                     # [113,32]
                        ], dim=0)  # [5,113,32]

                        features.append(feature)

                    feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]

                    # Target is the next multi_step xi timesteps
                    target = torch.tensor(target_seq, dtype=torch.float32)  # [multi_step,113,32]

                    samples.append({
                        'id': id,
                        'input': feature_sequence,  # [5,5,113,32]
                        'target': target  # [20,113,32]
                    })
            else:
                # For test set, you might not have targets, or handle similarly
                if Nt < self.seq_len:
                    print(f"Not enough timesteps for ID {id} in test set. Required: {self.seq_len}, Available: {Nt}")
                    continue

                theta_seq = theta[:self.seq_len]
                ustar_seq = ustar[:self.seq_len]
                xi_seq = xi[:self.seq_len]

                # Stack features per time step: [seq_len, 5, 113,32]
                features = []
                for i in range(self.seq_len):
                    theta_i = theta_seq[i]  # [113,32]
                    ustar_i = ustar_seq[i]
                    xi_i = xi_seq[i]
                    u_i = u
                    alpha_i = alpha

                    # Convert scalar features to tensors and tile
                    u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]
                    alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]

                    # Stack all features
                    feature = torch.stack([
                        torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]
                        torch.tensor(theta_i, dtype=torch.float32),  # [113,32]
                        torch.tensor(xi_i, dtype=torch.float32),     # [113,32]
                        u_tensor.squeeze(0),                        # [113,32]
                        alpha_tensor.squeeze(0)                     # [113,32]
                    ], dim=0)  # [5,113,32]

                    features.append(feature)

                feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]

                samples.append({
                    'id': id,
                    'input': feature_sequence,  # [5,5,113,32]
                    'target': None  # No target for test set
                })

        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        if self.is_train:
            return sample['input'], sample['target']  # [3,5,113,32], [1,113,32]
        else:
            return sample['input']  # Return only the input for the test set

class FireDataset(Dataset):
    def __init__(self, csv_data, data_dir, seq_len=3, is_train=True):
        """
        Args:
            csv_data (str or DataFrame): Path to the csv file with annotations, or the DataFrame itself.
            data_dir (str): Directory with all the .dat files.
            seq_len (int): Number of past timesteps to use as input.
            is_train (bool): Flag indicating training or testing mode.
        """
        if isinstance(csv_data, str):
            self.data_info = pd.read_csv(csv_data)
        else:
            self.data_info = csv_data  # Accept DataFrame directly

        self.data_dir = data_dir
        self.seq_len = seq_len
        self.is_train = is_train
        self.samples = self._create_samples()

    def _create_samples(self):
        samples = []
        for _, row in self.data_info.iterrows():
            id = row['id']
            u = row['u']
            alpha = row['alpha']
            Nt = row['Nt']
            # Load filenames
            theta_path = os.path.join(self.data_dir, row['theta_filename'])
            ustar_path = os.path.join(self.data_dir, row['ustar_filename'])
            xi_path = os.path.join(self.data_dir, row['xi_filename'])

            # Check if files exist
            if not os.path.exists(theta_path) or not os.path.exists(ustar_path) or not os.path.exists(xi_path):
                print(f"Missing files for ID {id}. Skipping.")
                continue

            # Load .dat files (assuming binary format)
            try:
                theta = np.fromfile(theta_path, dtype=np.float32).reshape(Nt, 113, 32)
                ustar = np.fromfile(ustar_path, dtype=np.float32).reshape(Nt, 113, 32)
                xi = np.fromfile(xi_path, dtype=np.float32).reshape(Nt, 113, 32)
            except ValueError as e:
                print(f"Error reshaping files for ID {id}: {e}")
                continue

            if self.is_train:
                # Ensure there are enough timesteps to create at least one sample
                if Nt < self.seq_len + 1:
                    print(f"Not enough timesteps for ID {id}. Required: {self.seq_len + 1}, Available: {Nt}")
                    continue

                # Create multiple samples using sliding window
                for t in range(Nt - self.seq_len):
                    # Input sequences: theta, ustar, xi for seq_len timesteps
                    theta_seq = theta[t:t+self.seq_len]  # Shape: [seq_len, 113, 32]
                    ustar_seq = ustar[t:t+self.seq_len]
                    xi_seq = xi[t:t+self.seq_len]

                    # Stack features per time step: [seq_len, 5, 113, 32]
                    # Each time step has channels: [ustar, theta, xi, u, alpha]
                    # u and alpha are scalar features, tiled to [1, 113, 32] each
                    features = []
                    for i in range(self.seq_len):
                        theta_i = theta_seq[i]  # [113,32]
                        ustar_i = ustar_seq[i]
                        xi_i = xi_seq[i]
                        u_i = u
                        alpha_i = alpha

                        # Convert scalar features to tensors and tile
                        u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]
                        alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]

                        # Stack all features
                        feature = torch.stack([
                            torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]
                            torch.tensor(theta_i, dtype=torch.float32),  # [113,32]
                            torch.tensor(xi_i, dtype=torch.float32),     # [113,32]
                            u_tensor.squeeze(0),                        # [113,32]
                            alpha_tensor.squeeze(0)                     # [113,32]
                        ], dim=0)  # [5,113,32]

                        features.append(feature)

                    feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]

                    # Target is the next xi timestep
                    target = torch.tensor(xi[t + self.seq_len], dtype=torch.float32).unsqueeze(0)  # [1,113,32]

                    samples.append({
                        'id': id,
                        'input': feature_sequence,  # [3,5,113,32]
                        'target': target  # [1,113,32]
                    })
            else:
                # For test set, create a single sample with the initial seq_len timesteps
                if Nt < self.seq_len:
                    print(f"Not enough timesteps for ID {id} in test set. Required: {self.seq_len}, Available: {Nt}")
                    continue

                theta_seq = theta[:self.seq_len]
                ustar_seq = ustar[:self.seq_len]
                xi_seq = xi[:self.seq_len]

                # Stack features per time step: [seq_len, 5, 113, 32]
                features = []
                for i in range(self.seq_len):
                    theta_i = theta_seq[i]  # [113,32]
                    ustar_i = ustar_seq[i]
                    xi_i = xi_seq[i]
                    u_i = u
                    alpha_i = alpha

                    # Convert scalar features to tensors and tile
                    u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]
                    alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]

                    # Stack all features
                    feature = torch.stack([
                        torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]
                        torch.tensor(theta_i, dtype=torch.float32),  # [113,32]
                        torch.tensor(xi_i, dtype=torch.float32),     # [113,32]
                        u_tensor.squeeze(0),                        # [113,32]
                        alpha_tensor.squeeze(0)                     # [113,32]
                    ], dim=0)  # [5,113,32]

                    features.append(feature)

                feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]

                samples.append({
                    'id': id,
                    'input': feature_sequence,  # [3,5,113,32]
                    'target': None  # No target for test set
                })

        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        if self.is_train:
            return sample['input'], sample['target']  # [3,5,113,32], [1,113,32]
        else:
            return sample['input'], None  # [3,5,113,32], None

class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)
        self.key_conv   = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)
        self.gamma      = nn.Parameter(torch.zeros(1))
        self.softmax    = nn.Softmax(dim=-1)

    def forward(self, x):
        """
            inputs :
                x : input feature maps (B X C X H X W)
            returns :
                out : self attention value + input feature
                attention: B X (H*W) X (H*W)
        """
        m_batchsize, C, width, height = x.size()
        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)  # B X N X C'
        proj_key    = self.key_conv(x).view(m_batchsize, -1, width*height)  # B X C' X N
        energy      = torch.bmm(proj_query, proj_key)  # batch matrix-matrix product: B X N X N
        attention   = self.softmax(energy)  # B X N X N
        proj_value  = self.value_conv(x).view(m_batchsize, -1, width*height)  # B X C X N

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B X C X N
        out = out.view(m_batchsize, C, width, height)

        out = self.gamma * out + x
        return out

# 5. Define the Self-Attention ConvLSTM Classes
class SelfAttentionConvLSTMCell(nn.Module):
    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):
        super(SelfAttentionConvLSTMCell, self).__init__()
        padding = kernel_size // 2
        # Standard ConvLSTMCell
        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=padding, bias=bias)
        # Self-Attention
        self.attention = SelfAttention(hidden_dim)
        self.hidden_dim = hidden_dim

    def forward(self, x, h_prev, c_prev):
        combined = torch.cat([x, h_prev], dim=1)  # [batch, input_dim + hidden_dim, H, W]
        conv_output = self.conv(combined)
        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_dim, dim=1)

        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)

        c_next = f * c_prev + i * g
        h_next = o * torch.tanh(c_next)

        # Apply Self-Attention
        h_next = self.attention(h_next)

        return h_next, c_next

class SelfAttentionConvLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):
        super(SelfAttentionConvLSTM, self).__init__()
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim

        self.cell_list = nn.ModuleList([
            SelfAttentionConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, kernel_size)
            for i in range(num_layers)
        ])

    def forward(self, x):
        # x shape: [batch_size, seq_len, channels, height, width]
        b, seq_len, _, h, w = x.size()
        h_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]
        c_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]

        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :, :, :]  # [batch, channels, H, W]
            h_t[0], c_t[0] = self.cell_list[0](x_t, h_t[0], c_t[0])
            for layer in range(1, self.num_layers):
                h_t[layer], c_t[layer] = self.cell_list[layer](h_t[layer - 1], h_t[layer], c_t[layer])
            outputs.append(h_t[-1])  # [batch, hidden_dim, H, W]

        outputs = torch.stack(outputs, dim=1)  # [batch, seq_len, hidden_dim, H, W]
        return outputs, (h_t, c_t)

class SelfAttentionConvLSTM_Model(nn.Module):
    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):
        super(SelfAttentionConvLSTM_Model, self).__init__()
        self.selfatt_convlstm = SelfAttentionConvLSTM(input_dim, hidden_dim, kernel_size, num_layers)
        self.conv_out = nn.Conv2d(hidden_dim, 1, kernel_size=1)

    def forward(self, x):
        # x shape: [batch_size, seq_len, channels, height, width]
        outputs, _ = self.selfatt_convlstm(x)
        # Get the output from the last time step
        last_output = outputs[:, -1, :, :, :]  # [batch, hidden_dim, H, W]
        out = self.conv_out(last_output)  # [batch, 1, H, W]
        return out

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    """
    Simplified Residual Block with a single convolution layer and skip connection.
    """
    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualBlock, self).__init__()
        self.activation = activation
        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)
        self.bn = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # Skip connection
        out = self.conv(x)
        out = self.bn(out)
        out += identity
        out = self.activation(out)
        return out

class ResidualCNN(nn.Module):
    """
    Simplified Residual CNN with fewer residual blocks and reduced width.
    """
    def __init__(self, in_channels=15, num_residual_blocks=3, out_channels=1, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualCNN, self).__init__()
        self.activation = activation
        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(in_channels)

        # Residual blocks reduced to 3
        self.residual_blocks = nn.Sequential(*[
            ResidualBlock(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)
            for _ in range(num_residual_blocks)
        ])

        # Output convolution layer
        self.output_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)

    def forward(self, x):
        # Reshape input to match expected size
        batch_size, seq_len, channels, height, width = x.size()
        x = x.view(batch_size, seq_len * channels, height, width)

        out = self.initial_conv(x)
        out = self.bn(out)
        out = self.activation(out)

        out = self.residual_blocks(out)
        out = self.output_conv(out)

        return out

import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock1(nn.Module):
    """
    Simplified Residual Block with a single convolution layer and skip connection.
    """
    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualBlock1, self).__init__()
        self.activation = activation
        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)
        self.bn = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # Skip connection
        out = self.conv(x)
        out = self.bn(out)
        out += identity
        out = self.activation(out)
        return out

class SelfAttention1(nn.Module):
    """
    Simple Self-Attention Module.
    """
    def __init__(self, in_dim):
        super(SelfAttention1, self).__init__()
        self.chanel_in = in_dim

        # Query, Key, Value transformations
        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)

        # Softmax for attention weights
        self.softmax = nn.Softmax(dim=-1)

        # Learnable scaling parameter
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        """
        Forward pass for self-attention.

        Args:
            x: Input feature maps (B x C x H x W)

        Returns:
            out: Self-attended feature maps
            attention: Attention map
        """
        m_batchsize, C, width, height = x.size()

        # Generate Query, Key, and Value matrices
        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)  # B x (W*H) x C'
        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)  # B x C' x (W*H)
        energy = torch.bmm(proj_query, proj_key)  # Batch matrix multiplication: B x (W*H) x (W*H)
        attention = self.softmax(energy)  # Apply softmax to get attention weights

        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)  # B x C x (W*H)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B x C x (W*H)
        out = out.view(m_batchsize, C, width, height)  # Reshape to original dimensions

        out = self.gamma * out + x  # Weighted sum with input (residual connection)
        return out, attention

class ResidualCNNWithSelfAttention(nn.Module):
    """
    Residual CNN with Self-Attention mechanism.
    """
    def __init__(self, in_channels=25, num_residual_blocks=3, out_channels=1, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualCNNWithSelfAttention, self).__init__()
        self.activation = activation
        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(in_channels)

        # Residual blocks reduced to 3
        self.residual_blocks = nn.Sequential(*[
            ResidualBlock1(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)
            for _ in range(num_residual_blocks)
        ])

        # Self-Attention module
        self.self_attention = SelfAttention1(in_dim=in_channels)

        # Output convolution layer
        self.output_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)

    def forward(self, x):
        # Reshape input to match expected size
        batch_size, seq_len, channels, height, width = x.size()
        x = x.view(batch_size, seq_len * channels, height, width)  # [batch, 25, 113, 32]

        out = self.initial_conv(x)
        out = self.bn(out)
        out = self.activation(out)

        out = self.residual_blocks(out)

        # Apply Self-Attention
        out, attention = self.self_attention(out)

        out = self.output_conv(out)

        return out

class ResidualBlock_20(nn.Module):
    """
    Simplified Residual Block with a single convolution layer and skip connection.
    """
    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualBlock_20, self).__init__()
        self.activation = activation
        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)
        self.bn = nn.BatchNorm2d(channels)

    def forward(self, x):
        identity = x  # Skip connection
        out = self.conv(x)
        out = self.bn(out)
        out += identity  # Add skip connection
        out = self.activation(out)
        return out

class ResidualCNN_20(nn.Module):
    """
    Residual CNN tailored for multi-step forecasting.
    """
    def __init__(self, in_channels=25, num_residual_blocks=4, out_channels=1, num_steps=20, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):
        super(ResidualCNN_20, self).__init__()
        self.num_steps = num_steps
        self.activation = activation
        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)
        self.bn = nn.BatchNorm2d(in_channels)

        # Residual blocks
        self.residual_blocks = nn.Sequential(*[
            ResidualBlock_20(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)
            for _ in range(num_residual_blocks)
        ])

        # Output convolution layer modified to output num_steps
        self.output_conv = nn.Conv2d(in_channels, out_channels * num_steps, kernel_size=1, stride=1, padding=0, bias=True)

    def forward(self, x):
        """
        Forward pass for ResidualCNN.

        Args:
            x (Tensor): Input tensor of shape [batch_size, seq_len, 5, 113, 32].

        Returns:
            Tensor: Output tensor of shape [batch_size, num_steps, out_channels, 113, 32].
        """
        batch_size, seq_len, channels, height, width = x.size()
        x = x.view(batch_size, seq_len * channels, height, width)  # [batch_size, 25, 113, 32]

        out = self.initial_conv(x)  # [batch_size, 25, 113, 32]
        out = self.bn(out)
        out = self.activation(out)

        out = self.residual_blocks(out)  # [batch_size, 25, 113, 32]
        out = self.output_conv(out)      # [batch_size, 20, 113, 32] assuming num_steps=20

        # Reshape to [batch_size, num_steps, out_channels, 113, 32]
        out = out.view(batch_size, self.num_steps, -1, height, width)

        return out

# Parameters for ConvLSTM
input_dim_conv = 5  # Number of input channels per time step
hidden_dim_conv = 64  # Number of filters in ConvLSTM
kernel_size_conv = 3
num_layers_conv = 2

# Parameters for Deformable ConvLSTM-SAC
hidden_dim_dconv = 64  # Number of filters in Deformable ConvLSTM
kernel_size_dconv = 3
num_layers_dconv = 2

# Parameters for Self-Attention ConvLSTM
hidden_dim_selfatt = 64  # Number of filters in Self-Attention ConvLSTM
kernel_size_selfatt = 3
num_layers_selfatt = 2

# Corrected Parameter for BiConvLSTM_UNet
hidden_dim_bi = 64
kernel_size_bi = 3
num_layers_bi = 2

# Parameters for 3D ConvLSTM
hidden_dim_3d = 64        # Number of hidden channels
kernel_size_3d = 3         # Kernel size for convolutions
num_layers_3d = 2          # Number of ConvLSTM3D layers

# 12. Create Training Dataset and Define Folds
df_train = pd.read_csv(train_csv)
simulations = df_train['id'].unique()
num_folds = len(simulations)  # 9 folds

# Shuffle simulations for randomness
np.random.seed(42)
shuffled_simulations = np.random.permutation(simulations)

# Assign each simulation to a fold (one simulation per fold)
folds = []
for sim in shuffled_simulations:
    folds.append([sim])  # Each fold has one simulation

criterion = nn.MSELoss()
saved_model_paths = []
fold_val_losses = {}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import os

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Gather all .pth model files in the directory
saved_model_paths = [

    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold1.pth',
    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold2.pth',
    #'/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold3.pth',
    #'/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold4.pth',
    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold5.pth',
    #'/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold6.pth',
    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold7.pth',
    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold8.pth',
    '/kaggle/input/residualcnn-seq-len-5/model_residualcnn_fold9.pth',

    #'/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold1.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold2.pth',
    #'/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold3.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold4.pth',
    #'/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold5.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold6.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold7.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold8.pth',
    '/kaggle/input/residualcnnsa-seq-len-5/model_residualcnnsa_fold9.pth',

    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold1.pth',
    #'/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold2.pth',
    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold3.pth',
    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold4.pth',
    #'/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold5.pth',
    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold6.pth',
    #'/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold7.pth',
    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold8.pth',
    '/kaggle/input/selfattconvlstm-seq-len-5/model_selfattconvlstm_fold9.pth',
]

if not saved_model_paths:
    raise ValueError(f"No .pth model files found in the directory.")

print(f"Found {len(saved_model_paths)} model files.")

# Function to parse the model_path and extract encoder_name, fold, and seq_len
def parse_model_path(model_path):
    dir_name = os.path.dirname(model_path)
    seq_len = SEQ_LEN  # Assume a constant seq_len is set in advance
    filename = os.path.basename(model_path)
    parts = filename.split('_')
    if len(parts) < 3:
        raise ValueError(f"Unexpected model filename format: {filename}")
    encoder_name = parts[1]
    fold_part = parts[2]
    fold_str = fold_part.split('.')[0]
    if not fold_str.startswith('fold'):
        raise ValueError(f"Unexpected fold format in filename: {fold_part}")
    fold = int(fold_str.replace('fold', ''))
    return encoder_name, fold, seq_len

selected_models = []

for model_path in saved_model_paths:
    try:
        encoder_name, fold, seq_len = parse_model_path(model_path)
    except ValueError as e:
        print(f"Skipping model {model_path}: {e}")
        continue

    selected_models.append({
        'model_path': model_path,
        'encoder_name': encoder_name,
        'fold': fold,
        'seq_len': seq_len
    })

if not selected_models:
    raise ValueError("No models found after parsing model paths.")

print(f"Selected {len(selected_models)} models for inference.")

# Test sequence length
test_seq_len = SEQ_LEN
print(f"Sequence length for testing: {test_seq_len}")

# Initialize the dataset
test_dataset = FireDataset(test_csv, test_data_dir, seq_len=test_seq_len, is_train=False)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)

# Extract IDs
ids = [sample['id'] for sample in test_dataset.samples]

predictions = []

# Load the selected models
models = []
model_weights = {
    'residualcnn': 1,
    'selfattconvlstm': 2,
    'residualcnnsa': 1,
}

for model_info in selected_models:
    model_path = model_info['model_path']
    encoder_name = model_info['encoder_name']
    fold = model_info['fold']
    seq_len = model_info['seq_len']

    print(f"Loading model: {model_path}, Encoder: {encoder_name}, Fold: {fold}, Seq_len: {seq_len}")

    in_channels = seq_len * 5

    if encoder_name == 'convlstm':
        model = ConvLSTMModel(input_dim=input_dim_conv, hidden_dim=hidden_dim_conv, kernel_size=kernel_size_conv, num_layers=num_layers_conv).to(device)
    elif encoder_name == 'residualcnn':
        model = ResidualCNN(in_channels=in_channels, num_residual_blocks=4).to(device)
    elif encoder_name == 'selfattconvlstm':
        model = SelfAttentionConvLSTM_Model(input_dim=input_dim_conv, hidden_dim=hidden_dim_selfatt, kernel_size=kernel_size_selfatt, num_layers=num_layers_selfatt).to(device)
    elif encoder_name == 'multiscaleconvlstm':
        model = MultiScaleConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers).to(device)
    elif encoder_name == 'residualcnnsa':
        model = ResidualCNNWithSelfAttention(
            in_channels=25,
            num_residual_blocks=4,
            out_channels=1,
            kernel_size=3,
            padding=1,
            activation=nn.ReLU(inplace=True)
        ).to(device)
    else:
        continue

    # Load state_dict
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
    except Exception as e:
        print(f"Error loading model {model_path}: {e}")
        continue

    model.eval()
    models.append({
        'model': model,
        'encoder_name': encoder_name,
        'seq_len': seq_len
    })

print(f"Loaded {len(models)} models successfully.")

# Prediction Loop
num_timesteps = NUM_TSTEPS  # Number of future timesteps to predict

for idx in range(len(test_dataset)):
    if (idx+1) % 100 == 0 or idx == 0:
        print(f"Processing sample {idx+1}/{len(test_dataset)}")

    input_seq, _ = test_dataset[idx]
    input_seq = input_seq.unsqueeze(0).to(device)

    preds_per_id = []

    for t in range(num_timesteps):
        ensemble_output = np.zeros((113, 32), dtype=np.float32)
        total_weight = 0

        for model_info in models:
            model = model_info['model']
            encoder_name = model_info['encoder_name']
            seq_len = model_info['seq_len']

            with torch.no_grad():
                selected_seq = input_seq[:, -seq_len:, :, :, :]

                # Model inference
                output = model(selected_seq).squeeze(0).squeeze(0).cpu().numpy()  # Shape: [113, 32]

                # Apply weight to the model's prediction
                weight = model_weights.get(encoder_name, 1.0)
                ensemble_output += weight * output
                total_weight += weight

        # Average the ensemble output
        ensemble_output /= total_weight

        preds_per_id.append(ensemble_output.flatten())  # Flattened prediction: [H*W]

        # Prepare the new prediction to append to the sequence
        new_pred = torch.from_numpy(ensemble_output).unsqueeze(0).unsqueeze(0).to(device)

        # Update the input_seq by shifting the sequence
        shifted_seq = input_seq[:, 1:, :, :, :]
        new_time_step = input_seq[:, -1, :, :, :].clone()
        new_time_step[:, 2, :, :] = new_pred.squeeze(0).squeeze(0)
        input_seq = torch.cat([shifted_seq, new_time_step.unsqueeze(1)], dim=1)

    # Concatenate all predictions for this ID
    preds_flat = np.concatenate(preds_per_id)  # Shape: [num_timesteps * H * W]
    predictions.append(preds_flat)
    predictions_np = np.array(predictions)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import os
from tqdm import tqdm

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Gather all .pth model files
saved_model_paths = [

    #'/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold1.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold2.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold3.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold4.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold5.pth',
    #'/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold6.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold7.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold8.pth',
    '/kaggle/input/residualcnn20-200-seq-len-5-1050/model_residualcnn_fold9.pth',
]

if not saved_model_paths:
    raise ValueError(f"No .pth model files found in the directory.")

print(f"Found {len(saved_model_paths)} model files.")

# Function to parse the model_path and extract encoder_name, fold, and seq_len
def parse_model_path(model_path):
    dir_name = os.path.dirname(model_path)
    seq_len = SEQ_LEN
    filename = os.path.basename(model_path)
    parts = filename.split('_')
    if len(parts) < 3:
        raise ValueError(f"Unexpected model filename format: {filename}")
    encoder_name = parts[1]
    fold_part = parts[2]
    fold_str = fold_part.split('.')[0]
    if not fold_str.startswith('fold'):
        raise ValueError(f"Unexpected fold format in filename: {fold_part}")
    fold = int(fold_str.replace('fold', ''))
    return encoder_name, fold, seq_len

selected_models = []

for model_path in saved_model_paths:
    try:
        encoder_name, fold, seq_len = parse_model_path(model_path)
    except ValueError as e:
        print(f"Skipping model {model_path}: {e}")
        continue

    selected_models.append({
        'model_path': model_path,
        'encoder_name': encoder_name,
        'fold': fold,
        'seq_len': seq_len
    })

if not selected_models:
    raise ValueError("No models found after parsing model paths.")

print(f"Selected {len(selected_models)} models for inference.")

# Define num_timesteps as desired
num_timesteps = NUM_TSTEPS
print(f"Number of timesteps for testing: {num_timesteps}")

# Test sequence length
test_seq_len = SEQ_LEN
print(f"Sequence length for testing: {test_seq_len}")

# Initialize the dataset
test_dataset = FireDataset_20(test_csv, test_data_dir, seq_len=test_seq_len, multi_step=num_timesteps, is_train=False)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)

# Extract IDs
ids = [sample['id'] for sample in test_loader.dataset.samples]

predictions = []

# Load the selected models
models = []
model_weights = {
    'residualcnn': 1,
}

for model_info in selected_models:
    model_path = model_info['model_path']
    encoder_name = model_info['encoder_name']
    fold = model_info['fold']
    seq_len = model_info['seq_len']

    print(f"Loading model: {model_path}, Encoder: {encoder_name}, Fold: {fold}, Seq_len: {seq_len}")

    in_channels = seq_len * 5

    # Initialize the model based on encoder_name
    if encoder_name == 'residualcnn':
        model = ResidualCNN_20(in_channels=in_channels, num_residual_blocks=4).to(device)
    elif encoder_name == 'convlstm':
        model = ConvLSTMModel(input_dim=input_dim_conv, hidden_dim=hidden_dim_conv, kernel_size=kernel_size_conv, num_layers=num_layers_conv).to(device)
    elif encoder_name == 'selfattconvlstm':
        model = SelfAttentionConvLSTM_Model(input_dim=input_dim_conv, hidden_dim=hidden_dim_selfatt, kernel_size=kernel_size_selfatt, num_layers=num_layers_selfatt).to(device)
    elif encoder_name == 'multiscaleconvlstm':
        model = MultiScaleConvLSTM(input_dim=input_dim, hidden_dim=hidden_dim, kernel_size=kernel_size, num_layers=num_layers).to(device)
    elif encoder_name == 'residualcnnsa':
            in_channels = 25            # seq_len=5 * features=5
            num_residual_blocks = 4
            num_steps = 20              # Number of future timesteps to predict
            out_channels = 1            # Number of output channels per timestep
            kernel_size = 3             # Kernel size for convolutions
            padding = 1                 # Padding for convolutions
            activation = nn.ReLU(inplace=True)
            model = ResidualCNNWithSelfAttentionMultiStep(
                in_channels=in_channels,
                num_residual_blocks=num_residual_blocks,
                num_steps=num_steps,
                out_channels=out_channels,
                kernel_size=kernel_size,
                padding=padding,
                activation=activation
            ).to(device)
    else:
        print(f"Unknown encoder_name: {encoder_name}. Skipping.")
        continue

    # Load state_dict
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
    except Exception as e:
        print(f"Error loading model {model_path}: {e}")
        continue

    model.eval()
    models.append({
        'model': model,
        'encoder_name': encoder_name,
        'seq_len': seq_len
    })

print(f"Loaded {len(models)} models successfully.")

# Prediction Loop
model_output_timesteps = 20  # Number of timesteps each model predicts
NT = num_timesteps // model_output_timesteps
remainder = num_timesteps % model_output_timesteps
total_iterations = NT + (1 if remainder > 0 else 0)
print(f"Total iterations required: {total_iterations}")

for idx, inputs in enumerate(tqdm(test_loader, desc="Predicting")):
    if (idx+1) % 100 == 0 or idx == 0:
        print(f"Processing sample {idx+1}/{len(test_dataset)}")

    preds_per_id = []
    input_seq = inputs.to(device)

    for t in range(total_iterations):
        ensemble_output = np.zeros((model_output_timesteps, 113, 32), dtype=np.float32)
        total_weight = 0

        for model_info in models:
            model = model_info['model']
            encoder_name = model_info['encoder_name']
            seq_len = model_info['seq_len']

            with torch.no_grad():

                selected_seq = input_seq[:, -seq_len:, :, :, :]

                # Model inference
                output = model(selected_seq)
#                 print("selected_seq")
#                 print(selected_seq)


                if encoder_name in ['residualcnn', 'residualcnnsa']:
                    output = output.view(1, model_output_timesteps, 1, 113, 32)


                # Apply weight to the model's prediction
                weight = model_weights.get(encoder_name, 1.0)
                ensemble_output += weight * output.reshape(20, 113, 32).cpu().numpy()
                total_weight += weight

        # Average the ensemble output
        ensemble_output /= total_weight

        # If this is the last iteration and remainder > 0, take only the first 'remainder' timesteps
        if (t == total_iterations - 1) and (remainder > 0):
            ensemble_output = ensemble_output[:remainder]  # Shape: [remainder, 113, 32]

        # Append predictions to preds_per_id
        preds_per_id.append(ensemble_output.reshape(-1))  # Flattened predictions
#         print("ensemble_output")
#         print(ensemble_output.reshape(-1))

        # Update input_seq with new predictions for next iteration
        # Prepare ensemble_output_tensor
        ensemble_output_tensor = torch.from_numpy(ensemble_output).unsqueeze(0).to(device)  # Shape: [1, num_new_timesteps, 113, 32]
        num_new_timesteps = ensemble_output_tensor.shape[1]  # Could be less than model_output_timesteps in last iteration

        # Initialize new_time_steps by repeating the last timestep's features
        new_time_steps = input_seq[:, -1, :, :, :].unsqueeze(1).repeat(1, num_new_timesteps, 1, 1, 1).clone()
        # new_time_steps shape: [1, num_new_timesteps, features, H, W]

        # Set the predicted xi (assuming xi is at index 2 in features)
        new_time_steps[:, :, 2, :, :] = ensemble_output_tensor  # Shape alignment

        # Append new_time_steps to input_seq
        input_seq = torch.cat([input_seq, new_time_steps], dim=1)  # Extend the sequence

        # Keep only the last seq_len timesteps
        input_seq = input_seq[:, -seq_len:, :, :, :]
#         print("input_seq")
#         print(input_seq)

    # Concatenate all predictions for this ID
    preds_flat = np.concatenate(preds_per_id)  # Shape: [num_timesteps * H * W]

    predictions.append(preds_flat)

# Convert predictions to numpy array
predictions_20_np = np.array(predictions)  # Shape: [num_samples, num_timesteps * H * W]
print(predictions_20_np.shape)
# Now, you can proceed to save the predictions or further process them as needed

# Ensure both arrays have the same shape before averaging
if predictions_20_np.shape == predictions_np.shape:
    final_predictions = predictions_20_np * 0.45 + predictions_np * 0.55
    print(final_predictions.shape)
else:
    print("Error: Shapes of the arrays do not match. Averaging not possible.")

#print(final_predictions)

# Prepare Submission
submission = pd.DataFrame(final_predictions)
submission.insert(0, 'id', ids)

# Generate column names from pixel_1 to pixel_72320(for 20 timesteps)
submission.columns = ['id'] + [f'pixel_{i}' for i in range(1, submission.shape[1])]

# Save submission
submission.to_csv('submission.csv', index=False)
print('Submission file saved as submission.csv')