{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e6018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:30.987161Z",
     "iopub.status.busy": "2024-10-17T09:11:30.986823Z",
     "iopub.status.idle": "2024-10-17T09:11:34.194661Z",
     "shell.execute_reply": "2024-10-17T09:11:34.193851Z"
    },
    "papermill": {
     "duration": 3.218318,
     "end_time": "2024-10-17T09:11:34.197259",
     "exception": false,
     "start_time": "2024-10-17T09:11:30.978941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the CUBLAS_WORKSPACE_CONFIG environment variable\n",
    "# ':4096:8' or ':16:8' can be used. Here, we use ':4096:8' as an example.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "SEED = 75485\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(torch.initial_seed() % 2**32)    \n",
    "    random.seed(torch.initial_seed() % 2**32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6cc756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:34.210787Z",
     "iopub.status.busy": "2024-10-17T09:11:34.210342Z",
     "iopub.status.idle": "2024-10-17T09:11:53.765784Z",
     "shell.execute_reply": "2024-10-17T09:11:53.764867Z"
    },
    "papermill": {
     "duration": 19.564525,
     "end_time": "2024-10-17T09:11:53.768090",
     "exception": false,
     "start_time": "2024-10-17T09:11:34.203565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch already installed.\n",
      "torchvision already installed.\n",
      "Installing segmentation-models-pytorch...\n",
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.25.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.3.0)\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.19.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.0)\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
      "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n",
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=e3c621c2c478b5a901610ab170d0efeed54c0ec665e981f8943695ce4e69e170\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "  Building wheel for pretrainedmodels (setup.py): started\n",
      "  Building wheel for pretrainedmodels (setup.py): finished with status 'done'\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=d385a54d46c8abc2b8bf1b88a9c203a89d3ad6ee71c88ad1cbe80546f37c9ea0\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.9\n",
      "    Uninstalling timm-1.0.9:\n",
      "      Successfully uninstalled timm-1.0.9\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n",
      "segmentation-models-pytorch installed successfully.\n",
      "efficientnet_pytorch already installed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import importlib  # Import importlib\n",
    "\n",
    "required_packages = ['torch', 'torchvision', 'segmentation-models-pytorch', 'efficientnet_pytorch']\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            # Check if the package is already installed (Python 3.7+)\n",
    "            importlib.import_module(package)\n",
    "            print(f\"{package} already installed.\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call(['pip', 'install', package])\n",
    "            print(f\"{package} installed successfully.\")\n",
    "\n",
    "install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beafa6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:53.784275Z",
     "iopub.status.busy": "2024-10-17T09:11:53.783864Z",
     "iopub.status.idle": "2024-10-17T09:11:53.791431Z",
     "shell.execute_reply": "2024-10-17T09:11:53.790524Z"
    },
    "papermill": {
     "duration": 0.017884,
     "end_time": "2024-10-17T09:11:53.793478",
     "exception": false,
     "start_time": "2024-10-17T09:11:53.775594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv = '/kaggle/input/2024-flame-ai-challenge/dataset/train.csv'\n",
    "train_data_dir = '/kaggle/input/2024-flame-ai-challenge/dataset/train'\n",
    "test_csv = '/kaggle/input/2024-flame-ai-challenge/dataset/test.csv'\n",
    "test_data_dir = '/kaggle/input/2024-flame-ai-challenge/dataset/test'\n",
    "\n",
    "SEQ_LEN = 5\n",
    "seq_len = SEQ_LEN\n",
    "batch_size = 4\n",
    "\n",
    "# Configuration Parameters\n",
    "use_early_stopping = False  # Set to False to disable early stopping\n",
    "patience = 5      # Number of epochs to wait for improvement\n",
    "delta = 0     # Minimum change to qualify as improvement\n",
    "\n",
    "# Define Model Names\n",
    "model_names = [\n",
    "#    'unet',\n",
    "#    'deeplabtemporal',\n",
    "#   'resnet50',\n",
    "#    'convlstm',\n",
    "#   'residualcnn',\n",
    "    'residualcnnsa',\n",
    "#    'residualdensecnn',\n",
    "#    'dconvlstmsac',\n",
    "#    'selfattconvlstm',\n",
    "#    'bidirectconvlstmunet',\n",
    "#    #'pinn-convlstm',\n",
    "#     '3dconvlstm',\n",
    "#    'convlstm_autoencoder',\n",
    "#    'stconvlstm_autoencoder',\n",
    "#    'spatiotemporal_transformer',\n",
    "#    'attentionunet',\n",
    "#    'tcnmodel',\n",
    "#    'hybridcnntransformer',\n",
    "#    'multiscaleconvlstm',\n",
    "#    'multiscaleselfattconvlstm',\n",
    "#    'multiscaleresidualcnn',\n",
    "#    'residualconvlstm',\n",
    "#    'residualunet',\n",
    "#    'biconvlstm'\n",
    "#    'spatiotemporalcnnmdn',\n",
    "]\n",
    "\n",
    "# Define default number of epochs\n",
    "num_epochs_default = 1\n",
    "\n",
    "# Update model_epochs to include all models or rely on the default\n",
    "model_epochs = {\n",
    "#     'unet': 100,\n",
    "#     'deeplabtemporal': 5,\n",
    "#      'resnet50': 50,\n",
    "#      'convlstm': 50,\n",
    "#      'residualcnn': 50,\n",
    "     'residualcnnsa': 50,\n",
    "#      'dconvlstmsac': 30,\n",
    "#      'selfattconvlstm': 40,\n",
    "#      'bidirectconvlstmunet': 50,\n",
    "#      'pinn-convlstm': 30,\n",
    "#      '3dconvlstm': 30,\n",
    "#     'convlstm_autoencoder': 30,\n",
    "#     'stconvlstm_autoencoder': 30,\n",
    "#     'spatiotemporal_transformer': 30  \n",
    "#     'attentionunet': 50,\n",
    "#     'tcnmodel': 50,\n",
    "#     'hybridcnntransformer': 50,\n",
    "#     'residualdensecnn': 50,\n",
    "#     'multiscaleconvlstm': 40,\n",
    "#     'multiscaleselfattconvlstm': 50,\n",
    "#     'multiscaleresidualcnn': 50,\n",
    "#     'residualconvlstm': 50,\n",
    "#     'residualunet': 50,\n",
    "#     'biconvlstm': 50,\n",
    "#     'spatiotemporalcnnmdn': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56779a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:53.808561Z",
     "iopub.status.busy": "2024-10-17T09:11:53.808242Z",
     "iopub.status.idle": "2024-10-17T09:11:57.170731Z",
     "shell.execute_reply": "2024-10-17T09:11:57.169729Z"
    },
    "papermill": {
     "duration": 3.373144,
     "end_time": "2024-10-17T09:11:57.173748",
     "exception": false,
     "start_time": "2024-10-17T09:11:53.800604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Import Necessary Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import (\n",
    "    resnet18, resnet34, resnet50, resnet101, resnet152,\n",
    "    ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights,\n",
    "    densenet121, DenseNet121_Weights,\n",
    "    mobilenet_v2, MobileNet_V2_Weights\n",
    ")\n",
    "from efficientnet_pytorch import EfficientNet  # Ensure this is installed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.ops as ops  # For DeformConv2d\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Try importing MobileNetV3 from timm\n",
    "try:\n",
    "    import timm\n",
    "    has_timm = True\n",
    "except ImportError:\n",
    "    has_timm = False\n",
    "    print(\"timm library not found. MobileNetV3 will not be available. Please install it using `pip install timm`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4f0323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.193397Z",
     "iopub.status.busy": "2024-10-17T09:11:57.193060Z",
     "iopub.status.idle": "2024-10-17T09:11:57.203627Z",
     "shell.execute_reply": "2024-10-17T09:11:57.202678Z"
    },
    "papermill": {
     "duration": 0.021505,
     "end_time": "2024-10-17T09:11:57.205647",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.184142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                           Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                        Default: 'checkpoint.pth'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # We want to maximize the score (minimize loss)\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a29799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.221223Z",
     "iopub.status.busy": "2024-10-17T09:11:57.220860Z",
     "iopub.status.idle": "2024-10-17T09:11:57.248322Z",
     "shell.execute_reply": "2024-10-17T09:11:57.247590Z"
    },
    "papermill": {
     "duration": 0.037576,
     "end_time": "2024-10-17T09:11:57.250273",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.212697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FireDataset(Dataset):\n",
    "    def __init__(self, csv_data, data_dir, seq_len=5, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_data (str or DataFrame): Path to the csv file with annotations, or the DataFrame itself.\n",
    "            data_dir (str): Directory with all the .dat files.\n",
    "            seq_len (int): Number of past timesteps to use as input.\n",
    "            is_train (bool): Flag indicating training or testing mode.\n",
    "        \"\"\"\n",
    "        if isinstance(csv_data, str):\n",
    "            self.data_info = pd.read_csv(csv_data)\n",
    "        else:\n",
    "            self.data_info = csv_data  # Accept DataFrame directly\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.is_train = is_train\n",
    "        self.samples = self._create_samples()\n",
    "\n",
    "    def _create_samples(self):\n",
    "        samples = []\n",
    "        for _, row in self.data_info.iterrows():\n",
    "            id = row['id']\n",
    "            u = row['u']\n",
    "            alpha = row['alpha']\n",
    "            Nt = row['Nt']\n",
    "            # Load filenames\n",
    "            theta_path = os.path.join(self.data_dir, row['theta_filename'])\n",
    "            ustar_path = os.path.join(self.data_dir, row['ustar_filename'])\n",
    "            xi_path = os.path.join(self.data_dir, row['xi_filename'])\n",
    "\n",
    "            # Check if files exist\n",
    "            if not os.path.exists(theta_path) or not os.path.exists(ustar_path) or not os.path.exists(xi_path):\n",
    "                print(f\"Missing files for ID {id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Load .dat files (assuming binary format)\n",
    "            try:\n",
    "                theta = np.fromfile(theta_path, dtype=np.float32).reshape(Nt, 113, 32)\n",
    "                ustar = np.fromfile(ustar_path, dtype=np.float32).reshape(Nt, 113, 32)\n",
    "                xi = np.fromfile(xi_path, dtype=np.float32).reshape(Nt, 113, 32)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error reshaping files for ID {id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if self.is_train:\n",
    "                # Ensure there are enough timesteps to create at least one sample\n",
    "                if Nt < self.seq_len + 1:\n",
    "                    print(f\"Not enough timesteps for ID {id}. Required: {self.seq_len + 1}, Available: {Nt}\")\n",
    "                    continue\n",
    "\n",
    "                # Create multiple samples using sliding window\n",
    "                for t in range(Nt - self.seq_len):\n",
    "                    # Input sequences: theta, ustar, xi for seq_len timesteps\n",
    "                    theta_seq = theta[t:t+self.seq_len]  # Shape: [seq_len, 113, 32]\n",
    "                    ustar_seq = ustar[t:t+self.seq_len]\n",
    "                    xi_seq = xi[t:t+self.seq_len]\n",
    "\n",
    "                    # Stack features per time step: [seq_len, 5, 113, 32]\n",
    "                    # Each time step has channels: [ustar, theta, xi, u, alpha]\n",
    "                    # u and alpha are scalar features, tiled to [1, 113, 32] each\n",
    "                    features = []\n",
    "                    for i in range(self.seq_len):\n",
    "                        theta_i = theta_seq[i]  # [113,32]\n",
    "                        ustar_i = ustar_seq[i]\n",
    "                        xi_i = xi_seq[i]\n",
    "                        u_i = u\n",
    "                        alpha_i = alpha\n",
    "\n",
    "                        # Convert scalar features to tensors and tile\n",
    "                        u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]\n",
    "                        alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]\n",
    "\n",
    "                        # Stack all features\n",
    "                        feature = torch.stack([\n",
    "                            torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]\n",
    "                            torch.tensor(theta_i, dtype=torch.float32),  # [113,32]\n",
    "                            torch.tensor(xi_i, dtype=torch.float32),     # [113,32]\n",
    "                            u_tensor.squeeze(0),                        # [113,32]\n",
    "                            alpha_tensor.squeeze(0)                     # [113,32]\n",
    "                        ], dim=0)  # [5,113,32]\n",
    "\n",
    "                        features.append(feature)\n",
    "                    \n",
    "                    feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]\n",
    "\n",
    "                    # Target is the next xi timestep\n",
    "                    target = torch.tensor(xi[t + self.seq_len], dtype=torch.float32).unsqueeze(0)  # [1,113,32]\n",
    "\n",
    "                    samples.append({\n",
    "                        'id': id,\n",
    "                        'input': feature_sequence,  # [3,5,113,32]\n",
    "                        'target': target  # [1,113,32]\n",
    "                    })\n",
    "            else:\n",
    "                # For test set, create a single sample with the initial seq_len timesteps\n",
    "                if Nt < self.seq_len:\n",
    "                    print(f\"Not enough timesteps for ID {id} in test set. Required: {self.seq_len}, Available: {Nt}\")\n",
    "                    continue\n",
    "\n",
    "                theta_seq = theta[:self.seq_len]\n",
    "                ustar_seq = ustar[:self.seq_len]\n",
    "                xi_seq = xi[:self.seq_len]\n",
    "\n",
    "                # Stack features per time step: [seq_len, 5, 113, 32]\n",
    "                features = []\n",
    "                for i in range(self.seq_len):\n",
    "                    theta_i = theta_seq[i]  # [113,32]\n",
    "                    ustar_i = ustar_seq[i]\n",
    "                    xi_i = xi_seq[i]\n",
    "                    u_i = u\n",
    "                    alpha_i = alpha\n",
    "\n",
    "                    # Convert scalar features to tensors and tile\n",
    "                    u_tensor = torch.tensor(u_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]\n",
    "                    alpha_tensor = torch.tensor(alpha_i, dtype=torch.float32).unsqueeze(0).unsqueeze(1).repeat(1, 113, 32)  # [1,113,32]\n",
    "\n",
    "                    # Stack all features\n",
    "                    feature = torch.stack([\n",
    "                        torch.tensor(ustar_i, dtype=torch.float32),  # [113,32]\n",
    "                        torch.tensor(theta_i, dtype=torch.float32),  # [113,32]\n",
    "                        torch.tensor(xi_i, dtype=torch.float32),     # [113,32]\n",
    "                        u_tensor.squeeze(0),                        # [113,32]\n",
    "                        alpha_tensor.squeeze(0)                     # [113,32]\n",
    "                    ], dim=0)  # [5,113,32]\n",
    "\n",
    "                    features.append(feature)\n",
    "                \n",
    "                feature_sequence = torch.stack(features, dim=0)  # [seq_len, 5, 113,32]\n",
    "\n",
    "                samples.append({\n",
    "                    'id': id,\n",
    "                    'input': feature_sequence,  # [3,5,113,32]\n",
    "                    'target': None  # No target for test set\n",
    "                })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.is_train:\n",
    "            return sample['input'], sample['target']  # [3,5,113,32], [1,113,32]\n",
    "        else:\n",
    "            return sample['input'], None  # [3,5,113,32], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f104bbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.265841Z",
     "iopub.status.busy": "2024-10-17T09:11:57.265165Z",
     "iopub.status.idle": "2024-10-17T09:11:57.279196Z",
     "shell.execute_reply": "2024-10-17T09:11:57.278393Z"
    },
    "papermill": {
     "duration": 0.024092,
     "end_time": "2024-10-17T09:11:57.281412",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.257320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Residual Block with a single convolution layer and skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Skip connection\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Residual CNN with fewer residual blocks and reduced width.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=15, num_residual_blocks=3, out_channels=1, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # Residual blocks reduced to 3\n",
    "        self.residual_blocks = nn.Sequential(*[\n",
    "            ResidualBlock(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)\n",
    "            for _ in range(num_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output convolution layer\n",
    "        self.output_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to match expected size\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, seq_len * channels, height, width)\n",
    "\n",
    "        out = self.initial_conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.residual_blocks(out)\n",
    "        out = self.output_conv(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86edc49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.299691Z",
     "iopub.status.busy": "2024-10-17T09:11:57.299363Z",
     "iopub.status.idle": "2024-10-17T09:11:57.312040Z",
     "shell.execute_reply": "2024-10-17T09:11:57.311199Z"
    },
    "papermill": {
     "duration": 0.024059,
     "end_time": "2024-10-17T09:11:57.313988",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.289929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock_20(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Residual Block with a single convolution layer and skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualBlock_20, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Skip connection\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out += identity  # Add skip connection\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class ResidualCNN_20(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual CNN tailored for multi-step forecasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=25, num_residual_blocks=4, out_channels=1, num_steps=20, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualCNN_20, self).__init__()\n",
    "        self.num_steps = num_steps\n",
    "        self.activation = activation\n",
    "        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "\n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(*[\n",
    "            ResidualBlock_20(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)\n",
    "            for _ in range(num_residual_blocks)\n",
    "        ])\n",
    "\n",
    "        # Output convolution layer modified to output num_steps\n",
    "        self.output_conv = nn.Conv2d(in_channels, out_channels * num_steps, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for ResidualCNN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [batch_size, seq_len, 5, 113, 32].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [batch_size, num_steps, out_channels, 113, 32].\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, seq_len * channels, height, width)  # [batch_size, 25, 113, 32]\n",
    "\n",
    "        out = self.initial_conv(x)  # [batch_size, 25, 113, 32]\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.residual_blocks(out)  # [batch_size, 25, 113, 32]\n",
    "        out = self.output_conv(out)      # [batch_size, 20, 113, 32] assuming num_steps=20\n",
    "\n",
    "        # Reshape to [batch_size, num_steps, out_channels, 113, 32]\n",
    "        out = out.view(batch_size, self.num_steps, -1, height, width)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d868a3fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.329372Z",
     "iopub.status.busy": "2024-10-17T09:11:57.329075Z",
     "iopub.status.idle": "2024-10-17T09:11:57.351323Z",
     "shell.execute_reply": "2024-10-17T09:11:57.350480Z"
    },
    "papermill": {
     "duration": 0.032171,
     "end_time": "2024-10-17T09:11:57.353172",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.321001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.key_conv   = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.gamma      = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax    = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps (B X C X H X W)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X (H*W) X (H*W)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)  # B X N X C'\n",
    "        proj_key    = self.key_conv(x).view(m_batchsize, -1, width*height)  # B X C' X N\n",
    "        energy      = torch.bmm(proj_query, proj_key)  # batch matrix-matrix product: B X N X N\n",
    "        attention   = self.softmax(energy)  # B X N X N\n",
    "        proj_value  = self.value_conv(x).view(m_batchsize, -1, width*height)  # B X C X N\n",
    "        \n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B X C X N\n",
    "        out = out.view(m_batchsize, C, width, height)\n",
    "        \n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# 5. Define the Self-Attention ConvLSTM Classes\n",
    "class SelfAttentionConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(SelfAttentionConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        # Standard ConvLSTMCell\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=padding, bias=bias)\n",
    "        # Self-Attention\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        combined = torch.cat([x, h_prev], dim=1)  # [batch, input_dim + hidden_dim, H, W]\n",
    "        conv_output = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_dim, dim=1)\n",
    "        \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        # Apply Self-Attention\n",
    "        h_next = self.attention(h_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "class SelfAttentionConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(SelfAttentionConvLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.cell_list = nn.ModuleList([\n",
    "            SelfAttentionConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, kernel_size)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, channels, height, width]\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "        h_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]  # [batch, channels, H, W]\n",
    "            h_t[0], c_t[0] = self.cell_list[0](x_t, h_t[0], c_t[0])\n",
    "            for layer in range(1, self.num_layers):\n",
    "                h_t[layer], c_t[layer] = self.cell_list[layer](h_t[layer - 1], h_t[layer], c_t[layer])\n",
    "            outputs.append(h_t[-1])  # [batch, hidden_dim, H, W]\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch, seq_len, hidden_dim, H, W]\n",
    "        return outputs, (h_t, c_t)\n",
    "\n",
    "class SelfAttentionConvLSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(SelfAttentionConvLSTM_Model, self).__init__()\n",
    "        self.selfatt_convlstm = SelfAttentionConvLSTM(input_dim, hidden_dim, kernel_size, num_layers)\n",
    "        self.conv_out = nn.Conv2d(hidden_dim, 1, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, channels, height, width]\n",
    "        outputs, _ = self.selfatt_convlstm(x)\n",
    "        # Get the output from the last time step\n",
    "        last_output = outputs[:, -1, :, :, :]  # [batch, hidden_dim, H, W]\n",
    "        out = self.conv_out(last_output)  # [batch, 1, H, W]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6576876c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.368219Z",
     "iopub.status.busy": "2024-10-17T09:11:57.367937Z",
     "iopub.status.idle": "2024-10-17T09:11:57.386773Z",
     "shell.execute_reply": "2024-10-17T09:11:57.385932Z"
    },
    "papermill": {
     "duration": 0.028656,
     "end_time": "2024-10-17T09:11:57.388744",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.360088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock1(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Residual Block with a single convolution layer and skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualBlock1, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=kernel_size, padding=padding, stride=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Skip connection\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class SelfAttention1(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Self-Attention Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention1, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        # Query, Key, Value transformations\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "\n",
    "        # Softmax for attention weights\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Learnable scaling parameter\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature maps (B x C x H x W)\n",
    "        \n",
    "        Returns:\n",
    "            out: Self-attended feature maps\n",
    "            attention: Attention map\n",
    "        \"\"\"\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "\n",
    "        # Generate Query, Key, and Value matrices\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)  # B x (W*H) x C'\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)  # B x C' x (W*H)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # Batch matrix multiplication: B x (W*H) x (W*H)\n",
    "        attention = self.softmax(energy)  # Apply softmax to get attention weights\n",
    "\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)  # B x C x (W*H)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B x C x (W*H)\n",
    "        out = out.view(m_batchsize, C, width, height)  # Reshape to original dimensions\n",
    "\n",
    "        out = self.gamma * out + x  # Weighted sum with input (residual connection)\n",
    "        return out, attention\n",
    "\n",
    "class ResidualCNNWithSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual CNN with Self-Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=25, num_residual_blocks=3, out_channels=1, kernel_size=3, padding=1, activation=nn.ReLU(inplace=True)):\n",
    "        super(ResidualCNNWithSelfAttention, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.initial_conv = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        # Residual blocks reduced to 3\n",
    "        self.residual_blocks = nn.Sequential(*[\n",
    "            ResidualBlock1(in_channels, kernel_size=kernel_size, padding=padding, activation=activation)\n",
    "            for _ in range(num_residual_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Self-Attention module\n",
    "        self.self_attention = SelfAttention1(in_dim=in_channels)\n",
    "        \n",
    "        # Output convolution layer\n",
    "        self.output_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to match expected size\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, seq_len * channels, height, width)  # [batch, 25, 113, 32]\n",
    "\n",
    "        out = self.initial_conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.residual_blocks(out)\n",
    "        \n",
    "        # Apply Self-Attention\n",
    "        out, attention = self.self_attention(out)\n",
    "        \n",
    "        out = self.output_conv(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3649a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.403965Z",
     "iopub.status.busy": "2024-10-17T09:11:57.403415Z",
     "iopub.status.idle": "2024-10-17T09:11:57.416844Z",
     "shell.execute_reply": "2024-10-17T09:11:57.415972Z"
    },
    "papermill": {
     "duration": 0.022942,
     "end_time": "2024-10-17T09:11:57.418673",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.395731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, encoder_name, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path, patience=5, delta=0, lambda_phy=1.0):\n",
    "    \"\"\"\n",
    "    Trains the model with Early Stopping and Physics-Informed Loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (nn.Module): Loss function for data loss.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        num_epochs (int): Maximum number of epochs to train.\n",
    "        device (torch.device): Device to train on.\n",
    "        model_save_path (str): Path to save the best model.\n",
    "        patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        lambda_phy (float): Weight for the physics-informed loss term.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The trained model loaded with the best weights.\n",
    "        best_val_loss (float): The best validation loss achieved.\n",
    "    \"\"\"\n",
    "    # Initialize EarlyStopping object if enabled\n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=patience, verbose=True, delta=delta, path=model_save_path)\n",
    "        print(\"Early Stopping is ENABLED.\")\n",
    "    else:\n",
    "        early_stopping = None\n",
    "        print(\"Early Stopping is DISABLED.\")\n",
    "        \n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)  # [batch, seq_len, 5, 113,32]           \n",
    "            targets = targets.to(device)  # [batch, 1, 113,32] or appropriate shape          \n",
    "            optimizer.zero_grad()           \n",
    "            outputs = model(inputs)      # [batch,1,113,32]   \n",
    "            data_loss = criterion(outputs, targets)\n",
    "            loss = data_loss    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)      # [batch,1,113,32]   \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "        if use_early_stopping:\n",
    "            early_stopping(epoch_val_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "        else:\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Model saved with val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        print(f'Best Val Loss: {early_stopping.val_loss_min:.4f}')\n",
    "    else:\n",
    "        print(f'Final Val Loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    return model, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd5a96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.433385Z",
     "iopub.status.busy": "2024-10-17T09:11:57.433099Z",
     "iopub.status.idle": "2024-10-17T09:11:57.521082Z",
     "shell.execute_reply": "2024-10-17T09:11:57.520311Z"
    },
    "papermill": {
     "duration": 0.097597,
     "end_time": "2024-10-17T09:11:57.523162",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.425565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters for Self-Attention ConvLSTM\n",
    "input_dim_selfatt = 5\n",
    "hidden_dim_selfatt = 64  # Number of filters in Self-Attention ConvLSTM\n",
    "kernel_size_selfatt = 3\n",
    "num_layers_selfatt = 5\n",
    "\n",
    "channels_per_timestep = 5\n",
    "\n",
    "# Create Training Dataset and Define Folds\n",
    "df_train = pd.read_csv(train_csv)\n",
    "simulations = df_train['id'].unique()\n",
    "num_folds = len(simulations)  # 9 folds\n",
    "\n",
    "# Shuffle simulations for randomness\n",
    "shuffled_simulations = np.random.permutation(simulations)\n",
    "\n",
    "# Assign each simulation to a fold (one simulation per fold)\n",
    "folds = []\n",
    "for sim in shuffled_simulations:\n",
    "    folds.append([sim])  # Each fold has one simulation\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "saved_model_paths = []\n",
    "fold_val_losses = {}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284f154",
   "metadata": {
    "papermill": {
     "duration": 0.006759,
     "end_time": "2024-10-17T09:11:57.537129",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.530370",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae69ef56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:11:57.552492Z",
     "iopub.status.busy": "2024-10-17T09:11:57.552127Z",
     "iopub.status.idle": "2024-10-17T10:03:25.801748Z",
     "shell.execute_reply": "2024-10-17T10:03:25.800347Z"
    },
    "papermill": {
     "duration": 3088.260069,
     "end_time": "2024-10-17T10:03:25.804025",
     "exception": false,
     "start_time": "2024-10-17T09:11:57.543956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/9\n",
      "\n",
      "Fold 1 - Validation 'id's:\n",
      "[633229]\n",
      "\n",
      "Fold 1 - Train 'id's:\n",
      "[804025 875935 930086 661713 868570  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0451\n",
      "Epoch 1/50, Val Loss: 0.0233\n",
      "Model saved with val loss: 0.0233\n",
      "Epoch 2/50, Train Loss: 0.0230\n",
      "Epoch 2/50, Val Loss: 0.0188\n",
      "Model saved with val loss: 0.0188\n",
      "Epoch 3/50, Train Loss: 0.0196\n",
      "Epoch 3/50, Val Loss: 0.0164\n",
      "Model saved with val loss: 0.0164\n",
      "Epoch 4/50, Train Loss: 0.0148\n",
      "Epoch 4/50, Val Loss: 0.0144\n",
      "Model saved with val loss: 0.0144\n",
      "Epoch 5/50, Train Loss: 0.0123\n",
      "Epoch 5/50, Val Loss: 0.0124\n",
      "Model saved with val loss: 0.0124\n",
      "Epoch 6/50, Train Loss: 0.0111\n",
      "Epoch 6/50, Val Loss: 0.0115\n",
      "Model saved with val loss: 0.0115\n",
      "Epoch 7/50, Train Loss: 0.0104\n",
      "Epoch 7/50, Val Loss: 0.0112\n",
      "Model saved with val loss: 0.0112\n",
      "Epoch 8/50, Train Loss: 0.0100\n",
      "Epoch 8/50, Val Loss: 0.0111\n",
      "Model saved with val loss: 0.0111\n",
      "Epoch 9/50, Train Loss: 0.0096\n",
      "Epoch 9/50, Val Loss: 0.0122\n",
      "Epoch 10/50, Train Loss: 0.0093\n",
      "Epoch 10/50, Val Loss: 0.0101\n",
      "Model saved with val loss: 0.0101\n",
      "Epoch 11/50, Train Loss: 0.0090\n",
      "Epoch 11/50, Val Loss: 0.0098\n",
      "Model saved with val loss: 0.0098\n",
      "Epoch 12/50, Train Loss: 0.0088\n",
      "Epoch 12/50, Val Loss: 0.0119\n",
      "Epoch 13/50, Train Loss: 0.0086\n",
      "Epoch 13/50, Val Loss: 0.0108\n",
      "Epoch 14/50, Train Loss: 0.0084\n",
      "Epoch 14/50, Val Loss: 0.0133\n",
      "Epoch 15/50, Train Loss: 0.0083\n",
      "Epoch 15/50, Val Loss: 0.0101\n",
      "Epoch 16/50, Train Loss: 0.0081\n",
      "Epoch 16/50, Val Loss: 0.0088\n",
      "Model saved with val loss: 0.0088\n",
      "Epoch 17/50, Train Loss: 0.0080\n",
      "Epoch 17/50, Val Loss: 0.0084\n",
      "Model saved with val loss: 0.0084\n",
      "Epoch 18/50, Train Loss: 0.0080\n",
      "Epoch 18/50, Val Loss: 0.0083\n",
      "Model saved with val loss: 0.0083\n",
      "Epoch 19/50, Train Loss: 0.0079\n",
      "Epoch 19/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 20/50, Train Loss: 0.0078\n",
      "Epoch 20/50, Val Loss: 0.0083\n",
      "Epoch 21/50, Train Loss: 0.0077\n",
      "Epoch 21/50, Val Loss: 0.0084\n",
      "Epoch 22/50, Train Loss: 0.0077\n",
      "Epoch 22/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 23/50, Train Loss: 0.0076\n",
      "Epoch 23/50, Val Loss: 0.0081\n",
      "Epoch 24/50, Train Loss: 0.0076\n",
      "Epoch 24/50, Val Loss: 0.0088\n",
      "Epoch 25/50, Train Loss: 0.0076\n",
      "Epoch 25/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 26/50, Train Loss: 0.0075\n",
      "Epoch 26/50, Val Loss: 0.0082\n",
      "Epoch 27/50, Train Loss: 0.0075\n",
      "Epoch 27/50, Val Loss: 0.0124\n",
      "Epoch 28/50, Train Loss: 0.0074\n",
      "Epoch 28/50, Val Loss: 0.0093\n",
      "Epoch 29/50, Train Loss: 0.0074\n",
      "Epoch 29/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 30/50, Train Loss: 0.0074\n",
      "Epoch 30/50, Val Loss: 0.0078\n",
      "Model saved with val loss: 0.0078\n",
      "Epoch 31/50, Train Loss: 0.0074\n",
      "Epoch 31/50, Val Loss: 0.0079\n",
      "Epoch 32/50, Train Loss: 0.0073\n",
      "Epoch 32/50, Val Loss: 0.1017\n",
      "Epoch 33/50, Train Loss: 0.0073\n",
      "Epoch 33/50, Val Loss: 0.0079\n",
      "Epoch 34/50, Train Loss: 0.0073\n",
      "Epoch 34/50, Val Loss: 0.0081\n",
      "Epoch 35/50, Train Loss: 0.0073\n",
      "Epoch 35/50, Val Loss: 0.0078\n",
      "Model saved with val loss: 0.0078\n",
      "Epoch 36/50, Train Loss: 0.0073\n",
      "Epoch 36/50, Val Loss: 0.0147\n",
      "Epoch 37/50, Train Loss: 0.0072\n",
      "Epoch 37/50, Val Loss: 0.0084\n",
      "Epoch 38/50, Train Loss: 0.0072\n",
      "Epoch 38/50, Val Loss: 0.0087\n",
      "Epoch 39/50, Train Loss: 0.0072\n",
      "Epoch 39/50, Val Loss: 0.0084\n",
      "Epoch 40/50, Train Loss: 0.0072\n",
      "Epoch 40/50, Val Loss: 0.0121\n",
      "Epoch 41/50, Train Loss: 0.0072\n",
      "Epoch 41/50, Val Loss: 0.0078\n",
      "Epoch 42/50, Train Loss: 0.0071\n",
      "Epoch 42/50, Val Loss: 0.0081\n",
      "Epoch 43/50, Train Loss: 0.0072\n",
      "Epoch 43/50, Val Loss: 0.0079\n",
      "Epoch 44/50, Train Loss: 0.0072\n",
      "Epoch 44/50, Val Loss: 0.0128\n",
      "Epoch 45/50, Train Loss: 0.0071\n",
      "Epoch 45/50, Val Loss: 0.0099\n",
      "Epoch 46/50, Train Loss: 0.0071\n",
      "Epoch 46/50, Val Loss: 0.0079\n",
      "Epoch 47/50, Train Loss: 0.0071\n",
      "Epoch 47/50, Val Loss: 0.0095\n",
      "Epoch 48/50, Train Loss: 0.0071\n",
      "Epoch 48/50, Val Loss: 0.0083\n",
      "Epoch 49/50, Train Loss: 0.0071\n",
      "Epoch 49/50, Val Loss: 0.0081\n",
      "Epoch 50/50, Train Loss: 0.0071\n",
      "Epoch 50/50, Val Loss: 0.0079\n",
      "Final Val Loss: 0.0078\n",
      "\n",
      "Fold 2/9\n",
      "\n",
      "Fold 2 - Validation 'id's:\n",
      "[804025]\n",
      "\n",
      "Fold 2 - Train 'id's:\n",
      "[875935 930086 661713 633229 868570  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.1784\n",
      "Epoch 1/50, Val Loss: 0.0260\n",
      "Model saved with val loss: 0.0260\n",
      "Epoch 2/50, Train Loss: 0.0272\n",
      "Epoch 2/50, Val Loss: 0.0213\n",
      "Model saved with val loss: 0.0213\n",
      "Epoch 3/50, Train Loss: 0.0232\n",
      "Epoch 3/50, Val Loss: 0.0187\n",
      "Model saved with val loss: 0.0187\n",
      "Epoch 4/50, Train Loss: 0.0202\n",
      "Epoch 4/50, Val Loss: 0.0164\n",
      "Model saved with val loss: 0.0164\n",
      "Epoch 5/50, Train Loss: 0.0159\n",
      "Epoch 5/50, Val Loss: 0.0119\n",
      "Model saved with val loss: 0.0119\n",
      "Epoch 6/50, Train Loss: 0.0130\n",
      "Epoch 6/50, Val Loss: 0.0103\n",
      "Model saved with val loss: 0.0103\n",
      "Epoch 7/50, Train Loss: 0.0117\n",
      "Epoch 7/50, Val Loss: 0.0094\n",
      "Model saved with val loss: 0.0094\n",
      "Epoch 8/50, Train Loss: 0.0109\n",
      "Epoch 8/50, Val Loss: 0.0113\n",
      "Epoch 9/50, Train Loss: 0.0104\n",
      "Epoch 9/50, Val Loss: 0.0095\n",
      "Epoch 10/50, Train Loss: 0.0099\n",
      "Epoch 10/50, Val Loss: 0.0084\n",
      "Model saved with val loss: 0.0084\n",
      "Epoch 11/50, Train Loss: 0.0096\n",
      "Epoch 11/50, Val Loss: 0.0078\n",
      "Model saved with val loss: 0.0078\n",
      "Epoch 12/50, Train Loss: 0.0093\n",
      "Epoch 12/50, Val Loss: 0.0083\n",
      "Epoch 13/50, Train Loss: 0.0091\n",
      "Epoch 13/50, Val Loss: 0.0085\n",
      "Epoch 14/50, Train Loss: 0.0088\n",
      "Epoch 14/50, Val Loss: 0.0073\n",
      "Model saved with val loss: 0.0073\n",
      "Epoch 15/50, Train Loss: 0.0086\n",
      "Epoch 15/50, Val Loss: 0.0071\n",
      "Model saved with val loss: 0.0071\n",
      "Epoch 16/50, Train Loss: 0.0085\n",
      "Epoch 16/50, Val Loss: 0.0074\n",
      "Epoch 17/50, Train Loss: 0.0084\n",
      "Epoch 17/50, Val Loss: 0.0071\n",
      "Epoch 18/50, Train Loss: 0.0082\n",
      "Epoch 18/50, Val Loss: 0.0067\n",
      "Model saved with val loss: 0.0067\n",
      "Epoch 19/50, Train Loss: 0.0081\n",
      "Epoch 19/50, Val Loss: 0.0070\n",
      "Epoch 20/50, Train Loss: 0.0080\n",
      "Epoch 20/50, Val Loss: 0.0082\n",
      "Epoch 21/50, Train Loss: 0.0079\n",
      "Epoch 21/50, Val Loss: 0.0073\n",
      "Epoch 22/50, Train Loss: 0.0079\n",
      "Epoch 22/50, Val Loss: 0.0067\n",
      "Model saved with val loss: 0.0067\n",
      "Epoch 23/50, Train Loss: 0.0078\n",
      "Epoch 23/50, Val Loss: 0.0159\n",
      "Epoch 24/50, Train Loss: 0.0078\n",
      "Epoch 24/50, Val Loss: 0.0066\n",
      "Model saved with val loss: 0.0066\n",
      "Epoch 25/50, Train Loss: 0.0077\n",
      "Epoch 25/50, Val Loss: 0.0068\n",
      "Epoch 26/50, Train Loss: 0.0077\n",
      "Epoch 26/50, Val Loss: 0.0064\n",
      "Model saved with val loss: 0.0064\n",
      "Epoch 27/50, Train Loss: 0.0076\n",
      "Epoch 27/50, Val Loss: 0.0066\n",
      "Epoch 28/50, Train Loss: 0.0076\n",
      "Epoch 28/50, Val Loss: 0.0076\n",
      "Epoch 29/50, Train Loss: 0.0076\n",
      "Epoch 29/50, Val Loss: 0.0077\n",
      "Epoch 30/50, Train Loss: 0.0075\n",
      "Epoch 30/50, Val Loss: 0.0063\n",
      "Model saved with val loss: 0.0063\n",
      "Epoch 31/50, Train Loss: 0.0075\n",
      "Epoch 31/50, Val Loss: 0.0061\n",
      "Model saved with val loss: 0.0061\n",
      "Epoch 32/50, Train Loss: 0.0075\n",
      "Epoch 32/50, Val Loss: 0.0062\n",
      "Epoch 33/50, Train Loss: 0.0075\n",
      "Epoch 33/50, Val Loss: 0.0061\n",
      "Model saved with val loss: 0.0061\n",
      "Epoch 34/50, Train Loss: 0.0074\n",
      "Epoch 34/50, Val Loss: 0.0093\n",
      "Epoch 35/50, Train Loss: 0.0074\n",
      "Epoch 35/50, Val Loss: 0.0062\n",
      "Epoch 36/50, Train Loss: 0.0074\n",
      "Epoch 36/50, Val Loss: 0.0472\n",
      "Epoch 37/50, Train Loss: 0.0074\n",
      "Epoch 37/50, Val Loss: 0.0061\n",
      "Epoch 38/50, Train Loss: 0.0074\n",
      "Epoch 38/50, Val Loss: 0.0062\n",
      "Epoch 39/50, Train Loss: 0.0074\n",
      "Epoch 39/50, Val Loss: 0.0060\n",
      "Model saved with val loss: 0.0060\n",
      "Epoch 40/50, Train Loss: 0.0073\n",
      "Epoch 40/50, Val Loss: 0.0086\n",
      "Epoch 41/50, Train Loss: 0.0073\n",
      "Epoch 41/50, Val Loss: 0.0062\n",
      "Epoch 42/50, Train Loss: 0.0073\n",
      "Epoch 42/50, Val Loss: 0.0072\n",
      "Epoch 43/50, Train Loss: 0.0073\n",
      "Epoch 43/50, Val Loss: 0.0071\n",
      "Epoch 44/50, Train Loss: 0.0073\n",
      "Epoch 44/50, Val Loss: 0.0110\n",
      "Epoch 45/50, Train Loss: 0.0073\n",
      "Epoch 45/50, Val Loss: 0.0064\n",
      "Epoch 46/50, Train Loss: 0.0073\n",
      "Epoch 46/50, Val Loss: 0.0101\n",
      "Epoch 47/50, Train Loss: 0.0072\n",
      "Epoch 47/50, Val Loss: 0.0060\n",
      "Model saved with val loss: 0.0060\n",
      "Epoch 48/50, Train Loss: 0.0072\n",
      "Epoch 48/50, Val Loss: 0.0080\n",
      "Epoch 49/50, Train Loss: 0.0072\n",
      "Epoch 49/50, Val Loss: 0.0069\n",
      "Epoch 50/50, Train Loss: 0.0072\n",
      "Epoch 50/50, Val Loss: 0.0062\n",
      "Final Val Loss: 0.0060\n",
      "\n",
      "Fold 3/9\n",
      "\n",
      "Fold 3 - Validation 'id's:\n",
      "[930086]\n",
      "\n",
      "Fold 3 - Train 'id's:\n",
      "[804025 875935 661713 633229 868570  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0482\n",
      "Epoch 1/50, Val Loss: 0.0283\n",
      "Model saved with val loss: 0.0283\n",
      "Epoch 2/50, Train Loss: 0.0227\n",
      "Epoch 2/50, Val Loss: 0.0237\n",
      "Model saved with val loss: 0.0237\n",
      "Epoch 3/50, Train Loss: 0.0187\n",
      "Epoch 3/50, Val Loss: 0.0237\n",
      "Model saved with val loss: 0.0237\n",
      "Epoch 4/50, Train Loss: 0.0147\n",
      "Epoch 4/50, Val Loss: 0.0148\n",
      "Model saved with val loss: 0.0148\n",
      "Epoch 5/50, Train Loss: 0.0124\n",
      "Epoch 5/50, Val Loss: 0.0131\n",
      "Model saved with val loss: 0.0131\n",
      "Epoch 6/50, Train Loss: 0.0111\n",
      "Epoch 6/50, Val Loss: 0.0130\n",
      "Model saved with val loss: 0.0130\n",
      "Epoch 7/50, Train Loss: 0.0104\n",
      "Epoch 7/50, Val Loss: 0.0127\n",
      "Model saved with val loss: 0.0127\n",
      "Epoch 8/50, Train Loss: 0.0098\n",
      "Epoch 8/50, Val Loss: 0.0120\n",
      "Model saved with val loss: 0.0120\n",
      "Epoch 9/50, Train Loss: 0.0093\n",
      "Epoch 9/50, Val Loss: 0.0134\n",
      "Epoch 10/50, Train Loss: 0.0090\n",
      "Epoch 10/50, Val Loss: 0.0109\n",
      "Model saved with val loss: 0.0109\n",
      "Epoch 11/50, Train Loss: 0.0087\n",
      "Epoch 11/50, Val Loss: 0.0116\n",
      "Epoch 12/50, Train Loss: 0.0085\n",
      "Epoch 12/50, Val Loss: 0.0106\n",
      "Model saved with val loss: 0.0106\n",
      "Epoch 13/50, Train Loss: 0.0083\n",
      "Epoch 13/50, Val Loss: 0.0209\n",
      "Epoch 14/50, Train Loss: 0.0082\n",
      "Epoch 14/50, Val Loss: 0.0117\n",
      "Epoch 15/50, Train Loss: 0.0081\n",
      "Epoch 15/50, Val Loss: 0.0103\n",
      "Model saved with val loss: 0.0103\n",
      "Epoch 16/50, Train Loss: 0.0080\n",
      "Epoch 16/50, Val Loss: 0.0102\n",
      "Model saved with val loss: 0.0102\n",
      "Epoch 17/50, Train Loss: 0.0079\n",
      "Epoch 17/50, Val Loss: 0.0127\n",
      "Epoch 18/50, Train Loss: 0.0078\n",
      "Epoch 18/50, Val Loss: 0.0106\n",
      "Epoch 19/50, Train Loss: 0.0077\n",
      "Epoch 19/50, Val Loss: 0.0104\n",
      "Epoch 20/50, Train Loss: 0.0076\n",
      "Epoch 20/50, Val Loss: 0.0102\n",
      "Epoch 21/50, Train Loss: 0.0076\n",
      "Epoch 21/50, Val Loss: 0.0219\n",
      "Epoch 22/50, Train Loss: 0.0075\n",
      "Epoch 22/50, Val Loss: 0.0104\n",
      "Epoch 23/50, Train Loss: 0.0075\n",
      "Epoch 23/50, Val Loss: 0.0101\n",
      "Model saved with val loss: 0.0101\n",
      "Epoch 24/50, Train Loss: 0.0074\n",
      "Epoch 24/50, Val Loss: 0.0189\n",
      "Epoch 25/50, Train Loss: 0.0074\n",
      "Epoch 25/50, Val Loss: 0.0098\n",
      "Model saved with val loss: 0.0098\n",
      "Epoch 26/50, Train Loss: 0.0073\n",
      "Epoch 26/50, Val Loss: 0.0099\n",
      "Epoch 27/50, Train Loss: 0.0073\n",
      "Epoch 27/50, Val Loss: 0.0118\n",
      "Epoch 28/50, Train Loss: 0.0072\n",
      "Epoch 28/50, Val Loss: 0.0257\n",
      "Epoch 29/50, Train Loss: 0.0072\n",
      "Epoch 29/50, Val Loss: 0.0101\n",
      "Epoch 30/50, Train Loss: 0.0072\n",
      "Epoch 30/50, Val Loss: 0.0110\n",
      "Epoch 31/50, Train Loss: 0.0072\n",
      "Epoch 31/50, Val Loss: 0.0097\n",
      "Model saved with val loss: 0.0097\n",
      "Epoch 32/50, Train Loss: 0.0071\n",
      "Epoch 32/50, Val Loss: 0.0122\n",
      "Epoch 33/50, Train Loss: 0.0071\n",
      "Epoch 33/50, Val Loss: 0.0100\n",
      "Epoch 34/50, Train Loss: 0.0071\n",
      "Epoch 34/50, Val Loss: 0.0098\n",
      "Epoch 35/50, Train Loss: 0.0071\n",
      "Epoch 35/50, Val Loss: 0.0101\n",
      "Epoch 36/50, Train Loss: 0.0070\n",
      "Epoch 36/50, Val Loss: 0.0127\n",
      "Epoch 37/50, Train Loss: 0.0070\n",
      "Epoch 37/50, Val Loss: 0.0125\n",
      "Epoch 38/50, Train Loss: 0.0070\n",
      "Epoch 38/50, Val Loss: 0.0127\n",
      "Epoch 39/50, Train Loss: 0.0070\n",
      "Epoch 39/50, Val Loss: 0.0100\n",
      "Epoch 40/50, Train Loss: 0.0070\n",
      "Epoch 40/50, Val Loss: 0.0104\n",
      "Epoch 41/50, Train Loss: 0.0070\n",
      "Epoch 41/50, Val Loss: 0.0096\n",
      "Model saved with val loss: 0.0096\n",
      "Epoch 42/50, Train Loss: 0.0069\n",
      "Epoch 42/50, Val Loss: 0.0097\n",
      "Epoch 43/50, Train Loss: 0.0069\n",
      "Epoch 43/50, Val Loss: 0.0098\n",
      "Epoch 44/50, Train Loss: 0.0069\n",
      "Epoch 44/50, Val Loss: 0.0097\n",
      "Epoch 45/50, Train Loss: 0.0069\n",
      "Epoch 45/50, Val Loss: 0.0160\n",
      "Epoch 46/50, Train Loss: 0.0069\n",
      "Epoch 46/50, Val Loss: 0.0105\n",
      "Epoch 47/50, Train Loss: 0.0069\n",
      "Epoch 47/50, Val Loss: 0.0100\n",
      "Epoch 48/50, Train Loss: 0.0069\n",
      "Epoch 48/50, Val Loss: 0.0096\n",
      "Model saved with val loss: 0.0096\n",
      "Epoch 49/50, Train Loss: 0.0068\n",
      "Epoch 49/50, Val Loss: 0.0094\n",
      "Model saved with val loss: 0.0094\n",
      "Epoch 50/50, Train Loss: 0.0068\n",
      "Epoch 50/50, Val Loss: 0.0096\n",
      "Final Val Loss: 0.0094\n",
      "\n",
      "Fold 4/9\n",
      "\n",
      "Fold 4 - Validation 'id's:\n",
      "[220212]\n",
      "\n",
      "Fold 4 - Train 'id's:\n",
      "[804025 875935 930086 661713 633229 868570  16525 808631]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0678\n",
      "Epoch 1/50, Val Loss: 0.0309\n",
      "Model saved with val loss: 0.0309\n",
      "Epoch 2/50, Train Loss: 0.0257\n",
      "Epoch 2/50, Val Loss: 0.0249\n",
      "Model saved with val loss: 0.0249\n",
      "Epoch 3/50, Train Loss: 0.0218\n",
      "Epoch 3/50, Val Loss: 0.0218\n",
      "Model saved with val loss: 0.0218\n",
      "Epoch 4/50, Train Loss: 0.0178\n",
      "Epoch 4/50, Val Loss: 0.0172\n",
      "Model saved with val loss: 0.0172\n",
      "Epoch 5/50, Train Loss: 0.0140\n",
      "Epoch 5/50, Val Loss: 0.0138\n",
      "Model saved with val loss: 0.0138\n",
      "Epoch 6/50, Train Loss: 0.0120\n",
      "Epoch 6/50, Val Loss: 0.0121\n",
      "Model saved with val loss: 0.0121\n",
      "Epoch 7/50, Train Loss: 0.0109\n",
      "Epoch 7/50, Val Loss: 0.0112\n",
      "Model saved with val loss: 0.0112\n",
      "Epoch 8/50, Train Loss: 0.0102\n",
      "Epoch 8/50, Val Loss: 0.0170\n",
      "Epoch 9/50, Train Loss: 0.0097\n",
      "Epoch 9/50, Val Loss: 0.0098\n",
      "Model saved with val loss: 0.0098\n",
      "Epoch 10/50, Train Loss: 0.0093\n",
      "Epoch 10/50, Val Loss: 0.0117\n",
      "Epoch 11/50, Train Loss: 0.0090\n",
      "Epoch 11/50, Val Loss: 0.0117\n",
      "Epoch 12/50, Train Loss: 0.0088\n",
      "Epoch 12/50, Val Loss: 0.0096\n",
      "Model saved with val loss: 0.0096\n",
      "Epoch 13/50, Train Loss: 0.0086\n",
      "Epoch 13/50, Val Loss: 0.0116\n",
      "Epoch 14/50, Train Loss: 0.0085\n",
      "Epoch 14/50, Val Loss: 0.0091\n",
      "Model saved with val loss: 0.0091\n",
      "Epoch 15/50, Train Loss: 0.0083\n",
      "Epoch 15/50, Val Loss: 0.0087\n",
      "Model saved with val loss: 0.0087\n",
      "Epoch 16/50, Train Loss: 0.0082\n",
      "Epoch 16/50, Val Loss: 0.0104\n",
      "Epoch 17/50, Train Loss: 0.0081\n",
      "Epoch 17/50, Val Loss: 0.0087\n",
      "Model saved with val loss: 0.0087\n",
      "Epoch 18/50, Train Loss: 0.0080\n",
      "Epoch 18/50, Val Loss: 0.0093\n",
      "Epoch 19/50, Train Loss: 0.0079\n",
      "Epoch 19/50, Val Loss: 0.0093\n",
      "Epoch 20/50, Train Loss: 0.0078\n",
      "Epoch 20/50, Val Loss: 0.0084\n",
      "Model saved with val loss: 0.0084\n",
      "Epoch 21/50, Train Loss: 0.0078\n",
      "Epoch 21/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 22/50, Train Loss: 0.0077\n",
      "Epoch 22/50, Val Loss: 0.0077\n",
      "Model saved with val loss: 0.0077\n",
      "Epoch 23/50, Train Loss: 0.0077\n",
      "Epoch 23/50, Val Loss: 0.0086\n",
      "Epoch 24/50, Train Loss: 0.0076\n",
      "Epoch 24/50, Val Loss: 0.0076\n",
      "Model saved with val loss: 0.0076\n",
      "Epoch 25/50, Train Loss: 0.0076\n",
      "Epoch 25/50, Val Loss: 0.0094\n",
      "Epoch 26/50, Train Loss: 0.0076\n",
      "Epoch 26/50, Val Loss: 0.0080\n",
      "Epoch 27/50, Train Loss: 0.0075\n",
      "Epoch 27/50, Val Loss: 0.0084\n",
      "Epoch 28/50, Train Loss: 0.0075\n",
      "Epoch 28/50, Val Loss: 0.0094\n",
      "Epoch 29/50, Train Loss: 0.0075\n",
      "Epoch 29/50, Val Loss: 0.0098\n",
      "Epoch 30/50, Train Loss: 0.0074\n",
      "Epoch 30/50, Val Loss: 0.0085\n",
      "Epoch 31/50, Train Loss: 0.0074\n",
      "Epoch 31/50, Val Loss: 0.0082\n",
      "Epoch 32/50, Train Loss: 0.0074\n",
      "Epoch 32/50, Val Loss: 0.0147\n",
      "Epoch 33/50, Train Loss: 0.0073\n",
      "Epoch 33/50, Val Loss: 0.0074\n",
      "Model saved with val loss: 0.0074\n",
      "Epoch 34/50, Train Loss: 0.0073\n",
      "Epoch 34/50, Val Loss: 0.0073\n",
      "Model saved with val loss: 0.0073\n",
      "Epoch 35/50, Train Loss: 0.0073\n",
      "Epoch 35/50, Val Loss: 0.0076\n",
      "Epoch 36/50, Train Loss: 0.0073\n",
      "Epoch 36/50, Val Loss: 0.0074\n",
      "Epoch 37/50, Train Loss: 0.0073\n",
      "Epoch 37/50, Val Loss: 0.0073\n",
      "Epoch 38/50, Train Loss: 0.0072\n",
      "Epoch 38/50, Val Loss: 0.0074\n",
      "Epoch 39/50, Train Loss: 0.0072\n",
      "Epoch 39/50, Val Loss: 0.0076\n",
      "Epoch 40/50, Train Loss: 0.0072\n",
      "Epoch 40/50, Val Loss: 0.0072\n",
      "Model saved with val loss: 0.0072\n",
      "Epoch 41/50, Train Loss: 0.0072\n",
      "Epoch 41/50, Val Loss: 0.0080\n",
      "Epoch 42/50, Train Loss: 0.0072\n",
      "Epoch 42/50, Val Loss: 0.0073\n",
      "Epoch 43/50, Train Loss: 0.0072\n",
      "Epoch 43/50, Val Loss: 0.0093\n",
      "Epoch 44/50, Train Loss: 0.0072\n",
      "Epoch 44/50, Val Loss: 0.0072\n",
      "Epoch 45/50, Train Loss: 0.0071\n",
      "Epoch 45/50, Val Loss: 0.0073\n",
      "Epoch 46/50, Train Loss: 0.0071\n",
      "Epoch 46/50, Val Loss: 0.0072\n",
      "Epoch 47/50, Train Loss: 0.0071\n",
      "Epoch 47/50, Val Loss: 0.0085\n",
      "Epoch 48/50, Train Loss: 0.0071\n",
      "Epoch 48/50, Val Loss: 0.0072\n",
      "Epoch 49/50, Train Loss: 0.0071\n",
      "Epoch 49/50, Val Loss: 0.0072\n",
      "Model saved with val loss: 0.0072\n",
      "Epoch 50/50, Train Loss: 0.0071\n",
      "Epoch 50/50, Val Loss: 0.0083\n",
      "Final Val Loss: 0.0072\n",
      "\n",
      "Fold 5/9\n",
      "\n",
      "Fold 5 - Validation 'id's:\n",
      "[868570]\n",
      "\n",
      "Fold 5 - Train 'id's:\n",
      "[804025 875935 930086 661713 633229  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0590\n",
      "Epoch 1/50, Val Loss: 0.0268\n",
      "Model saved with val loss: 0.0268\n",
      "Epoch 2/50, Train Loss: 0.0243\n",
      "Epoch 2/50, Val Loss: 0.0229\n",
      "Model saved with val loss: 0.0229\n",
      "Epoch 3/50, Train Loss: 0.0205\n",
      "Epoch 3/50, Val Loss: 0.0209\n",
      "Model saved with val loss: 0.0209\n",
      "Epoch 4/50, Train Loss: 0.0156\n",
      "Epoch 4/50, Val Loss: 0.0192\n",
      "Model saved with val loss: 0.0192\n",
      "Epoch 5/50, Train Loss: 0.0123\n",
      "Epoch 5/50, Val Loss: 0.0169\n",
      "Model saved with val loss: 0.0169\n",
      "Epoch 6/50, Train Loss: 0.0109\n",
      "Epoch 6/50, Val Loss: 0.0122\n",
      "Model saved with val loss: 0.0122\n",
      "Epoch 7/50, Train Loss: 0.0101\n",
      "Epoch 7/50, Val Loss: 0.0133\n",
      "Epoch 8/50, Train Loss: 0.0096\n",
      "Epoch 8/50, Val Loss: 0.0113\n",
      "Model saved with val loss: 0.0113\n",
      "Epoch 9/50, Train Loss: 0.0093\n",
      "Epoch 9/50, Val Loss: 0.0110\n",
      "Model saved with val loss: 0.0110\n",
      "Epoch 10/50, Train Loss: 0.0090\n",
      "Epoch 10/50, Val Loss: 0.0103\n",
      "Model saved with val loss: 0.0103\n",
      "Epoch 11/50, Train Loss: 0.0088\n",
      "Epoch 11/50, Val Loss: 0.0103\n",
      "Model saved with val loss: 0.0103\n",
      "Epoch 12/50, Train Loss: 0.0086\n",
      "Epoch 12/50, Val Loss: 0.0102\n",
      "Model saved with val loss: 0.0102\n",
      "Epoch 13/50, Train Loss: 0.0084\n",
      "Epoch 13/50, Val Loss: 0.0099\n",
      "Model saved with val loss: 0.0099\n",
      "Epoch 14/50, Train Loss: 0.0083\n",
      "Epoch 14/50, Val Loss: 0.0113\n",
      "Epoch 15/50, Train Loss: 0.0082\n",
      "Epoch 15/50, Val Loss: 0.0104\n",
      "Epoch 16/50, Train Loss: 0.0080\n",
      "Epoch 16/50, Val Loss: 0.0110\n",
      "Epoch 17/50, Train Loss: 0.0079\n",
      "Epoch 17/50, Val Loss: 0.0110\n",
      "Epoch 18/50, Train Loss: 0.0078\n",
      "Epoch 18/50, Val Loss: 0.0098\n",
      "Model saved with val loss: 0.0098\n",
      "Epoch 19/50, Train Loss: 0.0077\n",
      "Epoch 19/50, Val Loss: 0.0105\n",
      "Epoch 20/50, Train Loss: 0.0077\n",
      "Epoch 20/50, Val Loss: 0.0094\n",
      "Model saved with val loss: 0.0094\n",
      "Epoch 21/50, Train Loss: 0.0076\n",
      "Epoch 21/50, Val Loss: 0.0093\n",
      "Model saved with val loss: 0.0093\n",
      "Epoch 22/50, Train Loss: 0.0076\n",
      "Epoch 22/50, Val Loss: 0.0100\n",
      "Epoch 23/50, Train Loss: 0.0075\n",
      "Epoch 23/50, Val Loss: 0.0095\n",
      "Epoch 24/50, Train Loss: 0.0075\n",
      "Epoch 24/50, Val Loss: 0.0104\n",
      "Epoch 25/50, Train Loss: 0.0074\n",
      "Epoch 25/50, Val Loss: 0.0092\n",
      "Model saved with val loss: 0.0092\n",
      "Epoch 26/50, Train Loss: 0.0074\n",
      "Epoch 26/50, Val Loss: 0.0091\n",
      "Model saved with val loss: 0.0091\n",
      "Epoch 27/50, Train Loss: 0.0074\n",
      "Epoch 27/50, Val Loss: 0.0092\n",
      "Epoch 28/50, Train Loss: 0.0073\n",
      "Epoch 28/50, Val Loss: 0.0119\n",
      "Epoch 29/50, Train Loss: 0.0073\n",
      "Epoch 29/50, Val Loss: 0.0090\n",
      "Model saved with val loss: 0.0090\n",
      "Epoch 30/50, Train Loss: 0.0073\n",
      "Epoch 30/50, Val Loss: 0.0099\n",
      "Epoch 31/50, Train Loss: 0.0072\n",
      "Epoch 31/50, Val Loss: 0.0111\n",
      "Epoch 32/50, Train Loss: 0.0072\n",
      "Epoch 32/50, Val Loss: 0.0094\n",
      "Epoch 33/50, Train Loss: 0.0072\n",
      "Epoch 33/50, Val Loss: 0.0100\n",
      "Epoch 34/50, Train Loss: 0.0072\n",
      "Epoch 34/50, Val Loss: 0.0090\n",
      "Epoch 35/50, Train Loss: 0.0072\n",
      "Epoch 35/50, Val Loss: 0.0095\n",
      "Epoch 36/50, Train Loss: 0.0071\n",
      "Epoch 36/50, Val Loss: 0.0120\n",
      "Epoch 37/50, Train Loss: 0.0071\n",
      "Epoch 37/50, Val Loss: 0.0093\n",
      "Epoch 38/50, Train Loss: 0.0071\n",
      "Epoch 38/50, Val Loss: 0.0097\n",
      "Epoch 39/50, Train Loss: 0.0071\n",
      "Epoch 39/50, Val Loss: 0.0092\n",
      "Epoch 40/50, Train Loss: 0.0071\n",
      "Epoch 40/50, Val Loss: 0.0091\n",
      "Epoch 41/50, Train Loss: 0.0071\n",
      "Epoch 41/50, Val Loss: 0.0088\n",
      "Model saved with val loss: 0.0088\n",
      "Epoch 42/50, Train Loss: 0.0070\n",
      "Epoch 42/50, Val Loss: 0.0090\n",
      "Epoch 43/50, Train Loss: 0.0070\n",
      "Epoch 43/50, Val Loss: 0.0092\n",
      "Epoch 44/50, Train Loss: 0.0070\n",
      "Epoch 44/50, Val Loss: 0.0090\n",
      "Epoch 45/50, Train Loss: 0.0070\n",
      "Epoch 45/50, Val Loss: 0.0089\n",
      "Epoch 46/50, Train Loss: 0.0070\n",
      "Epoch 46/50, Val Loss: 0.0093\n",
      "Epoch 47/50, Train Loss: 0.0070\n",
      "Epoch 47/50, Val Loss: 0.0101\n",
      "Epoch 48/50, Train Loss: 0.0070\n",
      "Epoch 48/50, Val Loss: 0.0091\n",
      "Epoch 49/50, Train Loss: 0.0070\n",
      "Epoch 49/50, Val Loss: 0.0089\n",
      "Epoch 50/50, Train Loss: 0.0070\n",
      "Epoch 50/50, Val Loss: 0.0091\n",
      "Final Val Loss: 0.0088\n",
      "\n",
      "Fold 6/9\n",
      "\n",
      "Fold 6 - Validation 'id's:\n",
      "[661713]\n",
      "\n",
      "Fold 6 - Train 'id's:\n",
      "[804025 875935 930086 633229 868570  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0472\n",
      "Epoch 1/50, Val Loss: 0.0239\n",
      "Model saved with val loss: 0.0239\n",
      "Epoch 2/50, Train Loss: 0.0230\n",
      "Epoch 2/50, Val Loss: 0.0188\n",
      "Model saved with val loss: 0.0188\n",
      "Epoch 3/50, Train Loss: 0.0164\n",
      "Epoch 3/50, Val Loss: 0.0136\n",
      "Model saved with val loss: 0.0136\n",
      "Epoch 4/50, Train Loss: 0.0131\n",
      "Epoch 4/50, Val Loss: 0.0118\n",
      "Model saved with val loss: 0.0118\n",
      "Epoch 5/50, Train Loss: 0.0117\n",
      "Epoch 5/50, Val Loss: 0.0130\n",
      "Epoch 6/50, Train Loss: 0.0108\n",
      "Epoch 6/50, Val Loss: 0.0112\n",
      "Model saved with val loss: 0.0112\n",
      "Epoch 7/50, Train Loss: 0.0102\n",
      "Epoch 7/50, Val Loss: 0.0100\n",
      "Model saved with val loss: 0.0100\n",
      "Epoch 8/50, Train Loss: 0.0099\n",
      "Epoch 8/50, Val Loss: 0.0133\n",
      "Epoch 9/50, Train Loss: 0.0095\n",
      "Epoch 9/50, Val Loss: 0.0105\n",
      "Epoch 10/50, Train Loss: 0.0093\n",
      "Epoch 10/50, Val Loss: 0.0090\n",
      "Model saved with val loss: 0.0090\n",
      "Epoch 11/50, Train Loss: 0.0091\n",
      "Epoch 11/50, Val Loss: 0.0084\n",
      "Model saved with val loss: 0.0084\n",
      "Epoch 12/50, Train Loss: 0.0088\n",
      "Epoch 12/50, Val Loss: 0.0080\n",
      "Model saved with val loss: 0.0080\n",
      "Epoch 13/50, Train Loss: 0.0087\n",
      "Epoch 13/50, Val Loss: 0.0096\n",
      "Epoch 14/50, Train Loss: 0.0086\n",
      "Epoch 14/50, Val Loss: 0.0074\n",
      "Model saved with val loss: 0.0074\n",
      "Epoch 15/50, Train Loss: 0.0084\n",
      "Epoch 15/50, Val Loss: 0.0085\n",
      "Epoch 16/50, Train Loss: 0.0083\n",
      "Epoch 16/50, Val Loss: 0.0073\n",
      "Model saved with val loss: 0.0073\n",
      "Epoch 17/50, Train Loss: 0.0082\n",
      "Epoch 17/50, Val Loss: 0.0075\n",
      "Epoch 18/50, Train Loss: 0.0081\n",
      "Epoch 18/50, Val Loss: 0.0071\n",
      "Model saved with val loss: 0.0071\n",
      "Epoch 19/50, Train Loss: 0.0080\n",
      "Epoch 19/50, Val Loss: 0.0070\n",
      "Model saved with val loss: 0.0070\n",
      "Epoch 20/50, Train Loss: 0.0080\n",
      "Epoch 20/50, Val Loss: 0.0069\n",
      "Model saved with val loss: 0.0069\n",
      "Epoch 21/50, Train Loss: 0.0079\n",
      "Epoch 21/50, Val Loss: 0.0071\n",
      "Epoch 22/50, Train Loss: 0.0079\n",
      "Epoch 22/50, Val Loss: 0.0069\n",
      "Model saved with val loss: 0.0069\n",
      "Epoch 23/50, Train Loss: 0.0078\n",
      "Epoch 23/50, Val Loss: 0.0068\n",
      "Model saved with val loss: 0.0068\n",
      "Epoch 24/50, Train Loss: 0.0077\n",
      "Epoch 24/50, Val Loss: 0.0069\n",
      "Epoch 25/50, Train Loss: 0.0077\n",
      "Epoch 25/50, Val Loss: 0.0077\n",
      "Epoch 26/50, Train Loss: 0.0077\n",
      "Epoch 26/50, Val Loss: 0.0069\n",
      "Epoch 27/50, Train Loss: 0.0077\n",
      "Epoch 27/50, Val Loss: 0.0071\n",
      "Epoch 28/50, Train Loss: 0.0076\n",
      "Epoch 28/50, Val Loss: 0.0071\n",
      "Epoch 29/50, Train Loss: 0.0076\n",
      "Epoch 29/50, Val Loss: 0.0074\n",
      "Epoch 30/50, Train Loss: 0.0076\n",
      "Epoch 30/50, Val Loss: 0.0067\n",
      "Model saved with val loss: 0.0067\n",
      "Epoch 31/50, Train Loss: 0.0075\n",
      "Epoch 31/50, Val Loss: 0.0080\n",
      "Epoch 32/50, Train Loss: 0.0075\n",
      "Epoch 32/50, Val Loss: 0.0066\n",
      "Model saved with val loss: 0.0066\n",
      "Epoch 33/50, Train Loss: 0.0075\n",
      "Epoch 33/50, Val Loss: 0.0066\n",
      "Epoch 34/50, Train Loss: 0.0075\n",
      "Epoch 34/50, Val Loss: 0.0078\n",
      "Epoch 35/50, Train Loss: 0.0075\n",
      "Epoch 35/50, Val Loss: 0.0066\n",
      "Epoch 36/50, Train Loss: 0.0074\n",
      "Epoch 36/50, Val Loss: 0.0070\n",
      "Epoch 37/50, Train Loss: 0.0074\n",
      "Epoch 37/50, Val Loss: 0.0067\n",
      "Epoch 38/50, Train Loss: 0.0074\n",
      "Epoch 38/50, Val Loss: 0.0065\n",
      "Model saved with val loss: 0.0065\n",
      "Epoch 39/50, Train Loss: 0.0074\n",
      "Epoch 39/50, Val Loss: 0.0309\n",
      "Epoch 40/50, Train Loss: 0.0073\n",
      "Epoch 40/50, Val Loss: 0.0065\n",
      "Epoch 41/50, Train Loss: 0.0073\n",
      "Epoch 41/50, Val Loss: 0.0065\n",
      "Epoch 42/50, Train Loss: 0.0073\n",
      "Epoch 42/50, Val Loss: 0.0064\n",
      "Model saved with val loss: 0.0064\n",
      "Epoch 43/50, Train Loss: 0.0073\n",
      "Epoch 43/50, Val Loss: 0.0068\n",
      "Epoch 44/50, Train Loss: 0.0073\n",
      "Epoch 44/50, Val Loss: 0.0072\n",
      "Epoch 45/50, Train Loss: 0.0073\n",
      "Epoch 45/50, Val Loss: 0.0072\n",
      "Epoch 46/50, Train Loss: 0.0073\n",
      "Epoch 46/50, Val Loss: 0.0065\n",
      "Epoch 47/50, Train Loss: 0.0072\n",
      "Epoch 47/50, Val Loss: 0.0067\n",
      "Epoch 48/50, Train Loss: 0.0072\n",
      "Epoch 48/50, Val Loss: 0.0093\n",
      "Epoch 49/50, Train Loss: 0.0072\n",
      "Epoch 49/50, Val Loss: 0.0064\n",
      "Model saved with val loss: 0.0064\n",
      "Epoch 50/50, Train Loss: 0.0072\n",
      "Epoch 50/50, Val Loss: 0.0067\n",
      "Final Val Loss: 0.0064\n",
      "\n",
      "Fold 7/9\n",
      "\n",
      "Fold 7 - Validation 'id's:\n",
      "[16525]\n",
      "\n",
      "Fold 7 - Train 'id's:\n",
      "[804025 875935 930086 661713 633229 868570 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.1004\n",
      "Epoch 1/50, Val Loss: 0.0315\n",
      "Model saved with val loss: 0.0315\n",
      "Epoch 2/50, Train Loss: 0.0253\n",
      "Epoch 2/50, Val Loss: 0.0254\n",
      "Model saved with val loss: 0.0254\n",
      "Epoch 3/50, Train Loss: 0.0216\n",
      "Epoch 3/50, Val Loss: 0.0216\n",
      "Model saved with val loss: 0.0216\n",
      "Epoch 4/50, Train Loss: 0.0178\n",
      "Epoch 4/50, Val Loss: 0.0203\n",
      "Model saved with val loss: 0.0203\n",
      "Epoch 5/50, Train Loss: 0.0147\n",
      "Epoch 5/50, Val Loss: 0.0156\n",
      "Model saved with val loss: 0.0156\n",
      "Epoch 6/50, Train Loss: 0.0132\n",
      "Epoch 6/50, Val Loss: 0.0147\n",
      "Model saved with val loss: 0.0147\n",
      "Epoch 7/50, Train Loss: 0.0121\n",
      "Epoch 7/50, Val Loss: 0.0123\n",
      "Model saved with val loss: 0.0123\n",
      "Epoch 8/50, Train Loss: 0.0114\n",
      "Epoch 8/50, Val Loss: 0.0133\n",
      "Epoch 9/50, Train Loss: 0.0108\n",
      "Epoch 9/50, Val Loss: 0.0140\n",
      "Epoch 10/50, Train Loss: 0.0103\n",
      "Epoch 10/50, Val Loss: 0.0116\n",
      "Model saved with val loss: 0.0116\n",
      "Epoch 11/50, Train Loss: 0.0099\n",
      "Epoch 11/50, Val Loss: 0.0100\n",
      "Model saved with val loss: 0.0100\n",
      "Epoch 12/50, Train Loss: 0.0096\n",
      "Epoch 12/50, Val Loss: 0.0094\n",
      "Model saved with val loss: 0.0094\n",
      "Epoch 13/50, Train Loss: 0.0094\n",
      "Epoch 13/50, Val Loss: 0.0090\n",
      "Model saved with val loss: 0.0090\n",
      "Epoch 14/50, Train Loss: 0.0091\n",
      "Epoch 14/50, Val Loss: 0.0094\n",
      "Epoch 15/50, Train Loss: 0.0090\n",
      "Epoch 15/50, Val Loss: 0.0092\n",
      "Epoch 16/50, Train Loss: 0.0088\n",
      "Epoch 16/50, Val Loss: 0.0083\n",
      "Model saved with val loss: 0.0083\n",
      "Epoch 17/50, Train Loss: 0.0086\n",
      "Epoch 17/50, Val Loss: 0.0088\n",
      "Epoch 18/50, Train Loss: 0.0085\n",
      "Epoch 18/50, Val Loss: 0.0080\n",
      "Model saved with val loss: 0.0080\n",
      "Epoch 19/50, Train Loss: 0.0084\n",
      "Epoch 19/50, Val Loss: 0.0084\n",
      "Epoch 20/50, Train Loss: 0.0083\n",
      "Epoch 20/50, Val Loss: 0.0131\n",
      "Epoch 21/50, Train Loss: 0.0081\n",
      "Epoch 21/50, Val Loss: 0.0078\n",
      "Model saved with val loss: 0.0078\n",
      "Epoch 22/50, Train Loss: 0.0080\n",
      "Epoch 22/50, Val Loss: 0.0076\n",
      "Model saved with val loss: 0.0076\n",
      "Epoch 23/50, Train Loss: 0.0079\n",
      "Epoch 23/50, Val Loss: 0.0075\n",
      "Model saved with val loss: 0.0075\n",
      "Epoch 24/50, Train Loss: 0.0079\n",
      "Epoch 24/50, Val Loss: 0.0145\n",
      "Epoch 25/50, Train Loss: 0.0078\n",
      "Epoch 25/50, Val Loss: 0.0075\n",
      "Model saved with val loss: 0.0075\n",
      "Epoch 26/50, Train Loss: 0.0077\n",
      "Epoch 26/50, Val Loss: 0.0111\n",
      "Epoch 27/50, Train Loss: 0.0077\n",
      "Epoch 27/50, Val Loss: 0.0073\n",
      "Model saved with val loss: 0.0073\n",
      "Epoch 28/50, Train Loss: 0.0077\n",
      "Epoch 28/50, Val Loss: 0.0089\n",
      "Epoch 29/50, Train Loss: 0.0076\n",
      "Epoch 29/50, Val Loss: 0.0094\n",
      "Epoch 30/50, Train Loss: 0.0076\n",
      "Epoch 30/50, Val Loss: 0.0073\n",
      "Epoch 31/50, Train Loss: 0.0075\n",
      "Epoch 31/50, Val Loss: 0.0070\n",
      "Model saved with val loss: 0.0070\n",
      "Epoch 32/50, Train Loss: 0.0075\n",
      "Epoch 32/50, Val Loss: 0.0071\n",
      "Epoch 33/50, Train Loss: 0.0075\n",
      "Epoch 33/50, Val Loss: 0.0072\n",
      "Epoch 34/50, Train Loss: 0.0074\n",
      "Epoch 34/50, Val Loss: 0.0078\n",
      "Epoch 35/50, Train Loss: 0.0074\n",
      "Epoch 35/50, Val Loss: 0.0069\n",
      "Model saved with val loss: 0.0069\n",
      "Epoch 36/50, Train Loss: 0.0074\n",
      "Epoch 36/50, Val Loss: 0.0080\n",
      "Epoch 37/50, Train Loss: 0.0074\n",
      "Epoch 37/50, Val Loss: 0.0072\n",
      "Epoch 38/50, Train Loss: 0.0074\n",
      "Epoch 38/50, Val Loss: 0.0068\n",
      "Model saved with val loss: 0.0068\n",
      "Epoch 39/50, Train Loss: 0.0074\n",
      "Epoch 39/50, Val Loss: 0.0227\n",
      "Epoch 40/50, Train Loss: 0.0073\n",
      "Epoch 40/50, Val Loss: 0.0074\n",
      "Epoch 41/50, Train Loss: 0.0073\n",
      "Epoch 41/50, Val Loss: 0.0069\n",
      "Epoch 42/50, Train Loss: 0.0073\n",
      "Epoch 42/50, Val Loss: 0.0071\n",
      "Epoch 43/50, Train Loss: 0.0073\n",
      "Epoch 43/50, Val Loss: 0.0069\n",
      "Epoch 44/50, Train Loss: 0.0073\n",
      "Epoch 44/50, Val Loss: 0.0070\n",
      "Epoch 45/50, Train Loss: 0.0072\n",
      "Epoch 45/50, Val Loss: 0.0070\n",
      "Epoch 46/50, Train Loss: 0.0072\n",
      "Epoch 46/50, Val Loss: 0.0067\n",
      "Model saved with val loss: 0.0067\n",
      "Epoch 47/50, Train Loss: 0.0072\n",
      "Epoch 47/50, Val Loss: 0.0090\n",
      "Epoch 48/50, Train Loss: 0.0072\n",
      "Epoch 48/50, Val Loss: 0.0066\n",
      "Model saved with val loss: 0.0066\n",
      "Epoch 49/50, Train Loss: 0.0072\n",
      "Epoch 49/50, Val Loss: 0.0069\n",
      "Epoch 50/50, Train Loss: 0.0072\n",
      "Epoch 50/50, Val Loss: 0.1193\n",
      "Final Val Loss: 0.0066\n",
      "\n",
      "Fold 8/9\n",
      "\n",
      "Fold 8 - Validation 'id's:\n",
      "[808631]\n",
      "\n",
      "Fold 8 - Train 'id's:\n",
      "[804025 875935 930086 661713 633229 868570  16525 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0546\n",
      "Epoch 1/50, Val Loss: 0.0329\n",
      "Model saved with val loss: 0.0329\n",
      "Epoch 2/50, Train Loss: 0.0225\n",
      "Epoch 2/50, Val Loss: 0.0252\n",
      "Model saved with val loss: 0.0252\n",
      "Epoch 3/50, Train Loss: 0.0174\n",
      "Epoch 3/50, Val Loss: 0.0274\n",
      "Epoch 4/50, Train Loss: 0.0139\n",
      "Epoch 4/50, Val Loss: 0.0170\n",
      "Model saved with val loss: 0.0170\n",
      "Epoch 5/50, Train Loss: 0.0123\n",
      "Epoch 5/50, Val Loss: 0.0168\n",
      "Model saved with val loss: 0.0168\n",
      "Epoch 6/50, Train Loss: 0.0113\n",
      "Epoch 6/50, Val Loss: 0.0134\n",
      "Model saved with val loss: 0.0134\n",
      "Epoch 7/50, Train Loss: 0.0106\n",
      "Epoch 7/50, Val Loss: 0.0152\n",
      "Epoch 8/50, Train Loss: 0.0100\n",
      "Epoch 8/50, Val Loss: 0.0118\n",
      "Model saved with val loss: 0.0118\n",
      "Epoch 9/50, Train Loss: 0.0097\n",
      "Epoch 9/50, Val Loss: 0.0117\n",
      "Model saved with val loss: 0.0117\n",
      "Epoch 10/50, Train Loss: 0.0094\n",
      "Epoch 10/50, Val Loss: 0.0117\n",
      "Model saved with val loss: 0.0117\n",
      "Epoch 11/50, Train Loss: 0.0091\n",
      "Epoch 11/50, Val Loss: 0.0116\n",
      "Model saved with val loss: 0.0116\n",
      "Epoch 12/50, Train Loss: 0.0089\n",
      "Epoch 12/50, Val Loss: 0.0161\n",
      "Epoch 13/50, Train Loss: 0.0087\n",
      "Epoch 13/50, Val Loss: 0.0141\n",
      "Epoch 14/50, Train Loss: 0.0085\n",
      "Epoch 14/50, Val Loss: 0.0198\n",
      "Epoch 15/50, Train Loss: 0.0084\n",
      "Epoch 15/50, Val Loss: 0.0098\n",
      "Model saved with val loss: 0.0098\n",
      "Epoch 16/50, Train Loss: 0.0083\n",
      "Epoch 16/50, Val Loss: 0.0257\n",
      "Epoch 17/50, Train Loss: 0.0082\n",
      "Epoch 17/50, Val Loss: 0.0114\n",
      "Epoch 18/50, Train Loss: 0.0081\n",
      "Epoch 18/50, Val Loss: 0.0090\n",
      "Model saved with val loss: 0.0090\n",
      "Epoch 19/50, Train Loss: 0.0079\n",
      "Epoch 19/50, Val Loss: 0.0090\n",
      "Model saved with val loss: 0.0090\n",
      "Epoch 20/50, Train Loss: 0.0079\n",
      "Epoch 20/50, Val Loss: 0.0085\n",
      "Model saved with val loss: 0.0085\n",
      "Epoch 21/50, Train Loss: 0.0078\n",
      "Epoch 21/50, Val Loss: 0.0091\n",
      "Epoch 22/50, Train Loss: 0.0078\n",
      "Epoch 22/50, Val Loss: 0.0090\n",
      "Epoch 23/50, Train Loss: 0.0077\n",
      "Epoch 23/50, Val Loss: 0.0082\n",
      "Model saved with val loss: 0.0082\n",
      "Epoch 24/50, Train Loss: 0.0076\n",
      "Epoch 24/50, Val Loss: 0.0080\n",
      "Model saved with val loss: 0.0080\n",
      "Epoch 25/50, Train Loss: 0.0076\n",
      "Epoch 25/50, Val Loss: 0.0083\n",
      "Epoch 26/50, Train Loss: 0.0076\n",
      "Epoch 26/50, Val Loss: 0.0078\n",
      "Model saved with val loss: 0.0078\n",
      "Epoch 27/50, Train Loss: 0.0075\n",
      "Epoch 27/50, Val Loss: 0.0086\n",
      "Epoch 28/50, Train Loss: 0.0075\n",
      "Epoch 28/50, Val Loss: 0.0093\n",
      "Epoch 29/50, Train Loss: 0.0075\n",
      "Epoch 29/50, Val Loss: 0.0081\n",
      "Epoch 30/50, Train Loss: 0.0074\n",
      "Epoch 30/50, Val Loss: 0.0079\n",
      "Epoch 31/50, Train Loss: 0.0074\n",
      "Epoch 31/50, Val Loss: 0.0086\n",
      "Epoch 32/50, Train Loss: 0.0074\n",
      "Epoch 32/50, Val Loss: 0.0083\n",
      "Epoch 33/50, Train Loss: 0.0074\n",
      "Epoch 33/50, Val Loss: 0.0077\n",
      "Model saved with val loss: 0.0077\n",
      "Epoch 34/50, Train Loss: 0.0073\n",
      "Epoch 34/50, Val Loss: 0.0076\n",
      "Model saved with val loss: 0.0076\n",
      "Epoch 35/50, Train Loss: 0.0073\n",
      "Epoch 35/50, Val Loss: 0.0077\n",
      "Epoch 36/50, Train Loss: 0.0073\n",
      "Epoch 36/50, Val Loss: 0.0100\n",
      "Epoch 37/50, Train Loss: 0.0073\n",
      "Epoch 37/50, Val Loss: 0.0075\n",
      "Model saved with val loss: 0.0075\n",
      "Epoch 38/50, Train Loss: 0.0072\n",
      "Epoch 38/50, Val Loss: 0.0076\n",
      "Epoch 39/50, Train Loss: 0.0072\n",
      "Epoch 39/50, Val Loss: 0.0075\n",
      "Model saved with val loss: 0.0075\n",
      "Epoch 40/50, Train Loss: 0.0072\n",
      "Epoch 40/50, Val Loss: 0.0085\n",
      "Epoch 41/50, Train Loss: 0.0072\n",
      "Epoch 41/50, Val Loss: 0.0096\n",
      "Epoch 42/50, Train Loss: 0.0072\n",
      "Epoch 42/50, Val Loss: 0.0104\n",
      "Epoch 43/50, Train Loss: 0.0072\n",
      "Epoch 43/50, Val Loss: 0.0074\n",
      "Model saved with val loss: 0.0074\n",
      "Epoch 44/50, Train Loss: 0.0072\n",
      "Epoch 44/50, Val Loss: 0.0080\n",
      "Epoch 45/50, Train Loss: 0.0071\n",
      "Epoch 45/50, Val Loss: 0.0246\n",
      "Epoch 46/50, Train Loss: 0.0071\n",
      "Epoch 46/50, Val Loss: 0.0097\n",
      "Epoch 47/50, Train Loss: 0.0071\n",
      "Epoch 47/50, Val Loss: 0.0079\n",
      "Epoch 48/50, Train Loss: 0.0071\n",
      "Epoch 48/50, Val Loss: 0.0078\n",
      "Epoch 49/50, Train Loss: 0.0071\n",
      "Epoch 49/50, Val Loss: 0.0090\n",
      "Epoch 50/50, Train Loss: 0.0071\n",
      "Epoch 50/50, Val Loss: 0.0205\n",
      "Final Val Loss: 0.0074\n",
      "\n",
      "Fold 9/9\n",
      "\n",
      "Fold 9 - Validation 'id's:\n",
      "[875935]\n",
      "\n",
      "Fold 9 - Train 'id's:\n",
      "[804025 930086 661713 633229 868570  16525 808631 220212]\n",
      "Training samples: 1160\n",
      "Validation samples: 145\n",
      "\n",
      "Training model with encoder residualcnnsa\n",
      "Training residualcnnsa for 50\n",
      "Early Stopping is DISABLED.\n",
      "Epoch 1/50, Train Loss: 0.0589\n",
      "Epoch 1/50, Val Loss: 0.0212\n",
      "Model saved with val loss: 0.0212\n",
      "Epoch 2/50, Train Loss: 0.0247\n",
      "Epoch 2/50, Val Loss: 0.0178\n",
      "Model saved with val loss: 0.0178\n",
      "Epoch 3/50, Train Loss: 0.0214\n",
      "Epoch 3/50, Val Loss: 0.0152\n",
      "Model saved with val loss: 0.0152\n",
      "Epoch 4/50, Train Loss: 0.0171\n",
      "Epoch 4/50, Val Loss: 0.0133\n",
      "Model saved with val loss: 0.0133\n",
      "Epoch 5/50, Train Loss: 0.0133\n",
      "Epoch 5/50, Val Loss: 0.0104\n",
      "Model saved with val loss: 0.0104\n",
      "Epoch 6/50, Train Loss: 0.0119\n",
      "Epoch 6/50, Val Loss: 0.0108\n",
      "Epoch 7/50, Train Loss: 0.0110\n",
      "Epoch 7/50, Val Loss: 0.0086\n",
      "Model saved with val loss: 0.0086\n",
      "Epoch 8/50, Train Loss: 0.0105\n",
      "Epoch 8/50, Val Loss: 0.0084\n",
      "Model saved with val loss: 0.0084\n",
      "Epoch 9/50, Train Loss: 0.0101\n",
      "Epoch 9/50, Val Loss: 0.0084\n",
      "Epoch 10/50, Train Loss: 0.0098\n",
      "Epoch 10/50, Val Loss: 0.0094\n",
      "Epoch 11/50, Train Loss: 0.0095\n",
      "Epoch 11/50, Val Loss: 0.0081\n",
      "Model saved with val loss: 0.0081\n",
      "Epoch 12/50, Train Loss: 0.0093\n",
      "Epoch 12/50, Val Loss: 0.0113\n",
      "Epoch 13/50, Train Loss: 0.0091\n",
      "Epoch 13/50, Val Loss: 0.0074\n",
      "Model saved with val loss: 0.0074\n",
      "Epoch 14/50, Train Loss: 0.0089\n",
      "Epoch 14/50, Val Loss: 0.0075\n",
      "Epoch 15/50, Train Loss: 0.0087\n",
      "Epoch 15/50, Val Loss: 0.0071\n",
      "Model saved with val loss: 0.0071\n",
      "Epoch 16/50, Train Loss: 0.0086\n",
      "Epoch 16/50, Val Loss: 0.0076\n",
      "Epoch 17/50, Train Loss: 0.0084\n",
      "Epoch 17/50, Val Loss: 0.0069\n",
      "Model saved with val loss: 0.0069\n",
      "Epoch 18/50, Train Loss: 0.0083\n",
      "Epoch 18/50, Val Loss: 0.0089\n",
      "Epoch 19/50, Train Loss: 0.0082\n",
      "Epoch 19/50, Val Loss: 0.0071\n",
      "Epoch 20/50, Train Loss: 0.0081\n",
      "Epoch 20/50, Val Loss: 0.0070\n",
      "Epoch 21/50, Train Loss: 0.0080\n",
      "Epoch 21/50, Val Loss: 0.0067\n",
      "Model saved with val loss: 0.0067\n",
      "Epoch 22/50, Train Loss: 0.0080\n",
      "Epoch 22/50, Val Loss: 0.0079\n",
      "Epoch 23/50, Train Loss: 0.0079\n",
      "Epoch 23/50, Val Loss: 0.0064\n",
      "Model saved with val loss: 0.0064\n",
      "Epoch 24/50, Train Loss: 0.0078\n",
      "Epoch 24/50, Val Loss: 0.0094\n",
      "Epoch 25/50, Train Loss: 0.0078\n",
      "Epoch 25/50, Val Loss: 0.0071\n",
      "Epoch 26/50, Train Loss: 0.0077\n",
      "Epoch 26/50, Val Loss: 0.0070\n",
      "Epoch 27/50, Train Loss: 0.0077\n",
      "Epoch 27/50, Val Loss: 0.0063\n",
      "Model saved with val loss: 0.0063\n",
      "Epoch 28/50, Train Loss: 0.0077\n",
      "Epoch 28/50, Val Loss: 0.0065\n",
      "Epoch 29/50, Train Loss: 0.0076\n",
      "Epoch 29/50, Val Loss: 0.0071\n",
      "Epoch 30/50, Train Loss: 0.0076\n",
      "Epoch 30/50, Val Loss: 0.0065\n",
      "Epoch 31/50, Train Loss: 0.0076\n",
      "Epoch 31/50, Val Loss: 0.0063\n",
      "Model saved with val loss: 0.0063\n",
      "Epoch 32/50, Train Loss: 0.0076\n",
      "Epoch 32/50, Val Loss: 0.0067\n",
      "Epoch 33/50, Train Loss: 0.0075\n",
      "Epoch 33/50, Val Loss: 0.0069\n",
      "Epoch 34/50, Train Loss: 0.0075\n",
      "Epoch 34/50, Val Loss: 0.0065\n",
      "Epoch 35/50, Train Loss: 0.0075\n",
      "Epoch 35/50, Val Loss: 0.0063\n",
      "Epoch 36/50, Train Loss: 0.0075\n",
      "Epoch 36/50, Val Loss: 0.0062\n",
      "Model saved with val loss: 0.0062\n",
      "Epoch 37/50, Train Loss: 0.0074\n",
      "Epoch 37/50, Val Loss: 0.0063\n",
      "Epoch 38/50, Train Loss: 0.0074\n",
      "Epoch 38/50, Val Loss: 0.0061\n",
      "Model saved with val loss: 0.0061\n",
      "Epoch 39/50, Train Loss: 0.0074\n",
      "Epoch 39/50, Val Loss: 0.0066\n",
      "Epoch 40/50, Train Loss: 0.0074\n",
      "Epoch 40/50, Val Loss: 0.0065\n",
      "Epoch 41/50, Train Loss: 0.0074\n",
      "Epoch 41/50, Val Loss: 0.0063\n",
      "Epoch 42/50, Train Loss: 0.0074\n",
      "Epoch 42/50, Val Loss: 0.0067\n",
      "Epoch 43/50, Train Loss: 0.0074\n",
      "Epoch 43/50, Val Loss: 0.0081\n",
      "Epoch 44/50, Train Loss: 0.0074\n",
      "Epoch 44/50, Val Loss: 0.0061\n",
      "Model saved with val loss: 0.0061\n",
      "Epoch 45/50, Train Loss: 0.0073\n",
      "Epoch 45/50, Val Loss: 0.0112\n",
      "Epoch 46/50, Train Loss: 0.0073\n",
      "Epoch 46/50, Val Loss: 0.0063\n",
      "Epoch 47/50, Train Loss: 0.0073\n",
      "Epoch 47/50, Val Loss: 0.0061\n",
      "Epoch 48/50, Train Loss: 0.0073\n",
      "Epoch 48/50, Val Loss: 0.0061\n",
      "Epoch 49/50, Train Loss: 0.0073\n",
      "Epoch 49/50, Val Loss: 0.0062\n",
      "Epoch 50/50, Train Loss: 0.0073\n",
      "Epoch 50/50, Val Loss: 0.0060\n",
      "Model saved with val loss: 0.0060\n",
      "Final Val Loss: 0.0060\n",
      "Fold 1 Average Validation Loss: 0.0078\n",
      "Fold 2 Average Validation Loss: 0.0060\n",
      "Fold 3 Average Validation Loss: 0.0094\n",
      "Fold 4 Average Validation Loss: 0.0072\n",
      "Fold 5 Average Validation Loss: 0.0088\n",
      "Fold 6 Average Validation Loss: 0.0064\n",
      "Fold 7 Average Validation Loss: 0.0066\n",
      "Fold 8 Average Validation Loss: 0.0074\n",
      "Fold 9 Average Validation Loss: 0.0060\n",
      "Overall Average Validation Loss: 0.0073\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, val_simulations in enumerate(folds):\n",
    "    print(f'\\nFold {fold_idx+1}/{num_folds}')\n",
    "    \n",
    "    val_mask = df_train['id'].isin(val_simulations)\n",
    "    train_mask = ~val_mask\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} - Validation 'id's:\")\n",
    "    print(df_train.loc[val_mask, 'id'].unique())  # Print unique IDs in validation set\n",
    "\n",
    "    print(f\"\\nFold {fold_idx + 1} - Train 'id's:\")\n",
    "    print(df_train.loc[train_mask, 'id'].unique())  # Print unique IDs in training set\n",
    "\n",
    "    # Create training and validation subsets\n",
    "    train_subset = FireDataset(df_train[train_mask], train_data_dir, seq_len=seq_len, is_train=True)\n",
    "    val_subset = FireDataset(df_train[val_mask], train_data_dir, seq_len=seq_len, is_train=True)\n",
    "\n",
    "    print(f'Training samples: {len(train_subset)}')\n",
    "    print(f'Validation samples: {len(val_subset)}')\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, worker_init_fn=seed_worker)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, worker_init_fn=seed_worker)\n",
    "\n",
    "    # For each model\n",
    "    for encoder_name in model_names:\n",
    "        print(f'\\nTraining model with encoder {encoder_name}')\n",
    "        if encoder_name == 'residualcnn':\n",
    "            model = ResidualCNN(in_channels=seq_len * 5, num_residual_blocks=4).to(device)  # 5 channels per time step\n",
    "        elif encoder_name == 'selfattconvlstm':\n",
    "            model = SelfAttentionConvLSTM_Model(input_dim=input_dim_selfatt, hidden_dim=hidden_dim_selfatt, kernel_size=kernel_size_selfatt, num_layers=num_layers_selfatt).to(device)\n",
    "        elif encoder_name == 'residualcnnsa':\n",
    "            model = ResidualCNNWithSelfAttention(\n",
    "                in_channels=25,\n",
    "                num_residual_blocks=4,\n",
    "                out_channels=1,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                activation=nn.ReLU(inplace=True)\n",
    "            ).to(device)\n",
    "            \n",
    "        model_save_path = f'model_{encoder_name}_fold{fold_idx+1}.pth'\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Retrieve num_epochs for the current model\n",
    "        current_num_epochs = model_epochs.get(encoder_name, num_epochs_default)  # Fallback to default num_epochs if not specified\n",
    "        print(f\"Training {encoder_name} for {current_num_epochs}\")\n",
    "\n",
    "        # Train the model with Early Stopping\n",
    "        model, best_val_loss = train_model(\n",
    "            model=model,\n",
    "            encoder_name=encoder_name,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=current_num_epochs,\n",
    "            device=device,\n",
    "            model_save_path=model_save_path,\n",
    "            patience=patience,\n",
    "            delta=delta,\n",
    "            lambda_phy=1.0  # Weight for physics loss; adjust as needed\n",
    "        )\n",
    "        \n",
    "        fold_val_losses[(fold_idx + 1, encoder_name)] = best_val_loss\n",
    "        \n",
    "        # Append the model save path\n",
    "        saved_model_paths.append(model_save_path)\n",
    "\n",
    "# Print average score for each fold and across all models\n",
    "for fold_idx in range(num_folds):\n",
    "    fold_losses = [loss for (fold, model), loss in fold_val_losses.items() if fold == fold_idx + 1]\n",
    "    avg_fold_loss = np.mean(fold_losses) if fold_losses else 0.0  # Handle potential empty list\n",
    "    print(f\"Fold {fold_idx+1} Average Validation Loss: {avg_fold_loss:.4f}\")\n",
    "\n",
    "# Calculate and print overall average validation loss across all folds and models\n",
    "overall_avg_loss = np.mean(list(fold_val_losses.values()))\n",
    "print(f\"Overall Average Validation Loss: {overall_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c28c3d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T10:03:25.961678Z",
     "iopub.status.busy": "2024-10-17T10:03:25.961197Z",
     "iopub.status.idle": "2024-10-17T10:03:29.460262Z",
     "shell.execute_reply": "2024-10-17T10:03:29.458979Z"
    },
    "papermill": {
     "duration": 3.58083,
     "end_time": "2024-10-17T10:03:29.462856",
     "exception": false,
     "start_time": "2024-10-17T10:03:25.882026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca328374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T10:03:29.615457Z",
     "iopub.status.busy": "2024-10-17T10:03:29.615054Z",
     "iopub.status.idle": "2024-10-17T10:03:49.179654Z",
     "shell.execute_reply": "2024-10-17T10:03:49.178479Z"
    },
    "papermill": {
     "duration": 19.643565,
     "end_time": "2024-10-17T10:03:49.181848",
     "exception": false,
     "start_time": "2024-10-17T10:03:29.538283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: model_residualcnnsa_fold1.pth, Encoder: residualcnnsa, Fold: 1, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold2.pth, Encoder: residualcnnsa, Fold: 2, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold3.pth, Encoder: residualcnnsa, Fold: 3, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold4.pth, Encoder: residualcnnsa, Fold: 4, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold5.pth, Encoder: residualcnnsa, Fold: 5, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold6.pth, Encoder: residualcnnsa, Fold: 6, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold7.pth, Encoder: residualcnnsa, Fold: 7, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold8.pth, Encoder: residualcnnsa, Fold: 8, Seq_len: 5\n",
      "Loading model: model_residualcnnsa_fold9.pth, Encoder: residualcnnsa, Fold: 9, Seq_len: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/2786056410.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to parse the model_path and extract encoder_name, fold, and seq_len.\n",
    "def parse_model_path(model_path):\n",
    "    \"\"\"\n",
    "    Parses the model path to extract encoder_name and fold.\n",
    "    Assumes the model_path format: 'model_<encoder_name>_fold<fold>.pth'\n",
    "    Example: 'model_residualcnn_fold1.pth'\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(model_path)\n",
    "    parts = filename.split('_')\n",
    "\n",
    "    if len(parts) < 3:\n",
    "        raise ValueError(f\"Unexpected model filename format: {filename}\")\n",
    "\n",
    "    encoder_name = parts[1]\n",
    "\n",
    "    # Extract fold\n",
    "    fold_part = parts[2]  # e.g., 'fold1.pth'\n",
    "    fold_str = fold_part.split('.')[0]  # 'fold1'\n",
    "    fold = int(fold_str.replace('fold', ''))\n",
    "\n",
    "    # Define seq_len based on encoder_name using a predefined mapping\n",
    "    encoder_seq_len_mapping = {\n",
    "        'residualcnn': SEQ_LEN,\n",
    "        'residualcnnsa': SEQ_LEN,\n",
    "        'convlstm': SEQ_LEN,\n",
    "        'selfattconvlstm': SEQ_LEN,\n",
    "        '3dconvlstm': SEQ_LEN,\n",
    "        'dconvlstmsac': SEQ_LEN,\n",
    "        'bidirectconvlstmunet': SEQ_LEN,\n",
    "        'attentionunet': SEQ_LEN,\n",
    "        'deeplabtemporal': SEQ_LEN,\n",
    "        'unet': SEQ_LEN,\n",
    "        'residualdensecnn': SEQ_LEN,\n",
    "        'multiscaleconvlstm': SEQ_LEN,\n",
    "        'biconvlstm': SEQ_LEN,\n",
    "        'spatiotemporalcnnmdn': SEQ_LEN,\n",
    "    }\n",
    "\n",
    "    if encoder_name not in encoder_seq_len_mapping:\n",
    "        raise ValueError(f\"Unknown encoder_name '{encoder_name}', cannot determine seq_len.\")\n",
    "\n",
    "    seq_len = encoder_seq_len_mapping[encoder_name]\n",
    "\n",
    "    return encoder_name, fold, seq_len\n",
    "\n",
    "\n",
    "# Define the maximum sequence length required for your models\n",
    "test_seq_len = max([parse_model_path(model_path)[2] for model_path in saved_model_paths])  # e.g., 5\n",
    "test_dataset = FireDataset(test_csv, test_data_dir, seq_len=test_seq_len, is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, worker_init_fn=seed_worker)\n",
    "\n",
    "# Extract IDs\n",
    "ids = [sample['id'] for sample in test_dataset.samples]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "models = []\n",
    "model_seq_lens = []  # To store seq_len for each model\n",
    "\n",
    "for model_path in saved_model_paths:\n",
    "    try:\n",
    "        encoder_name, fold, seq_len = parse_model_path(model_path)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping model {model_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Loading model: {model_path}, Encoder: {encoder_name}, Fold: {fold}, Seq_len: {seq_len}\")\n",
    "\n",
    "    # Initialize the model with the correct in_channels based on seq_len\n",
    "    in_channels = seq_len * 5  # 5 channels per time step\n",
    "\n",
    "    if encoder_name == 'residualcnn':\n",
    "        model = ResidualCNN(in_channels=in_channels, num_residual_blocks=4).to(device)\n",
    "    elif encoder_name == 'selfattconvlstm':\n",
    "          model = SelfAttentionConvLSTM_Model(input_dim=input_dim_selfatt, hidden_dim=hidden_dim_selfatt, kernel_size=kernel_size_selfatt, num_layers=num_layers_selfatt).to(device)\n",
    "    elif encoder_name == 'residualcnnsa':\n",
    "            model = ResidualCNNWithSelfAttention(\n",
    "                in_channels=25,\n",
    "                num_residual_blocks=4,\n",
    "                out_channels=1,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                activation=nn.ReLU(inplace=True)\n",
    "            ).to(device)\n",
    "            \n",
    "    # Load state_dict\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    model_seq_lens.append(seq_len)  # Store the seq_len for this model\n",
    "\n",
    "\n",
    "# Prediction Loop\n",
    "num_timesteps = 20  # Number of future timesteps to predict\n",
    "\n",
    "for idx in range(len(test_dataset)):\n",
    "    # Extract the sample\n",
    "    input_seq, _ = test_dataset[idx]  # FireDataset returns (input_seq, None)\n",
    "    input_seq = input_seq.unsqueeze(0).to(device)  # Shape: [1, seq_len_max, 5, 113,32]\n",
    "\n",
    "    preds_per_id = []\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        ensemble_output = np.zeros((113,32), dtype=np.float32)\n",
    "        model_count = 0  # To count valid models for averaging\n",
    "\n",
    "        for model, seq_len in zip(models, model_seq_lens):\n",
    "            with torch.no_grad():\n",
    "                # Prepare input based on the model's seq_len\n",
    "                if input_seq.size(1) < seq_len:\n",
    "                    raise ValueError(f\"Input sequence length {input_seq.size(1)} is less than model's seq_len {seq_len}.\")\n",
    "\n",
    "                # Select the last 'seq_len' time steps\n",
    "                selected_seq = input_seq[:, -seq_len:, :, :, :]  # [1, seq_len, 5,113,32]\n",
    "                \n",
    "                output = model(selected_seq)  # [1,1,113,32]\n",
    "\n",
    "                # Ensure output shape consistency\n",
    "                if output.shape != (1, 1, 113, 32):\n",
    "                    raise ValueError(f\"Unexpected output shape: {output.shape}\")\n",
    "\n",
    "                ensemble_output += output.squeeze(0).squeeze(0).cpu().numpy()  # Shape: [113,32]\n",
    "                model_count += 1\n",
    "\n",
    "        if model_count == 0:\n",
    "            raise ValueError(\"No valid models were processed for averaging.\")\n",
    "\n",
    "        ensemble_output /= model_count  # Averaging over ensemble\n",
    "\n",
    "        preds_per_id.append(ensemble_output.flatten())  # Flattened prediction: [H*W]\n",
    "\n",
    "        # Prepare the new prediction to append to the sequence\n",
    "        new_pred = torch.from_numpy(ensemble_output).unsqueeze(0).unsqueeze(0).to(device)  # [1,1,113,32]\n",
    "\n",
    "        # Update the input_seq by shifting the sequence\n",
    "        shifted_seq = input_seq[:, 1:, :, :, :]  # [1, seq_len_max -1, 5,113,32]\n",
    "        new_time_step = input_seq[:, -1, :, :, :].clone()  # [1,5,113,32]\n",
    "        new_time_step[:, 2, :, :] = new_pred.squeeze(0).squeeze(0)  # Replace 'xi' channel\n",
    "        input_seq = torch.cat([shifted_seq, new_time_step.unsqueeze(1)], dim=1)  # [1, seq_len_max,5,113,32]\n",
    "\n",
    "    # Concatenate all predictions for this ID\n",
    "    preds_flat = np.concatenate(preds_per_id)  # Shape: [num_timesteps * H * W]\n",
    "    predictions.append(preds_flat)\n",
    "\n",
    "# Prepare Submission\n",
    "submission = pd.DataFrame(predictions)\n",
    "submission.insert(0, 'id', ids)\n",
    "expected_pixels = 113 * 32 * num_timesteps  # 72,320 for 20 timesteps\n",
    "submission.columns = ['id'] + [f'pixel_{i}' for i in range(1, submission.shape[1])]\n",
    "assert submission.shape == (len(test_dataset), 1 + expected_pixels), f\"Expected shape ({len(test_dataset)}, {1 + expected_pixels}), got {submission.shape}\"\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file saved as submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9748215,
     "sourceId": 85210,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3142.810731,
   "end_time": "2024-10-17T10:03:51.083849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-17T09:11:28.273118",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
